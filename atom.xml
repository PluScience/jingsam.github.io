<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>jingsam</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://jingsam.github.io/"/>
  <updated>2019-06-17T05:34:56.115Z</updated>
  <id>https://jingsam.github.io/</id>
  
  <author>
    <name>jingsam</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>利用反向代理实现内网服务器自动部署</title>
    <link href="https://jingsam.github.io/2019/06/14/deploy-with-autossh.html"/>
    <id>https://jingsam.github.io/2019/06/14/deploy-with-autossh.html</id>
    <published>2019-06-14T06:36:17.000Z</published>
    <updated>2019-06-17T05:34:56.115Z</updated>
    
    <content type="html"><![CDATA[<p>当前持续集成的流行，大大提高了开发迭代的速度。自动化部署作为持续集成的最后一公里，对于完成整个闭环至关重要。如果生产服务器部署至公网且开通了ssh端口，那么测试服务器完成测试打包后，可以很方便地利用ssh将部署包推送到生产服务器上完成部署。但是，如果生产服务器部署在内网，测试服务器部署在外网，那么测试服务器就无法得知生产服务器的IP，无法利用ssh远程登录实现自动部署。</p><p>本文利用ssh的反向代理技术，来实现内网服务器的自动部署。</p><h2 id="技术原理"><a href="#技术原理" class="headerlink" title="技术原理"></a>技术原理</h2><p>ssh一般用来客户端远程登录到服务器上，而ssh反向代理“反其道而行之”，由服务端主动发起请求连接客户端，然后在客户端打开一个端口，之后发往客户端的数据包将会转发到服务端。例如在服务端执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -NR 22022:localhost:22 clientUser@clientMachine</span><br></pre></td></tr></table></figure><p>连接成功后，发往客户端<code>22022</code>的数据包将被转发到服务端的<code>22</code>端口上。这时，如果客户端具有公网IP，那么我们就可以利用客户端机器作为跳板，远程登录到内网服务器上，命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -p 22022 serverUser@clientMachine</span><br></pre></td></tr></table></figure><p>通过ssh的反向代理技术，让内网服务器借用公网客户机的IP，实现内网服务器公网可见。在此基础上，实现内网服务器自动部署的原理如下图：</p><p><img src="/assets/ssh-reverse-proxy.png" alt=""></p><p>上述原理很简单，但具体操作起来，还是有许多细节要考虑，下面对此一一说明。</p><h2 id="持久化连接"><a href="#持久化连接" class="headerlink" title="持久化连接"></a>持久化连接</h2><p>通过ssh在第①步建立了生产服务器<code>prodServer</code>与跳板机<code>jumpServer</code>的连接隧道，但是这条隧道如果长时间没有数据包传输，那么ssh会主动关闭连接，之后测试服务器<code>testServer</code>就不能再通过跳板机连接到生产服务器。</p><p>为了保持生产服务器与跳板机的连接隧道不断开，需要ssh自动重连。ssh不支持自动重连功能，幸好有autossh帮我们完成这项工作。</p><p>autossh不属于系统自带工具，我们需要在生产服务器安装它，Ubuntu上的安装命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install autossh</span><br></pre></td></tr></table></figure><p>安装好之后，执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">autossh -M 0 -f -o <span class="string">"ServerAliveInterval=30"</span> -o <span class="string">"ServerAliveCountMax=3"</span> -o <span class="string">"ExitOnForwardFailure=yes"</span> -NR 22022:localhost:22 jump@jumpServer</span><br></pre></td></tr></table></figure><p>上述命令中，除了<code>-M 0</code>和<code>-f</code>是autossh的参数外，其他参数都原样传递给ssh，其中<code>-M 0</code>表示不另开端口监测ssh，<code>-f</code>表示后台运行。auotssh以前是靠<code>-M</code>另开一个端口发送心跳数据包，由于新版ssh（protocol 2）内建了心跳功能，所以不再推荐另开端口。上面的命令使用<code>ServerAliveInterval</code>和<code>ServerAliveCountMax</code>两个参数，表示客户端向服务端每30秒发送一次心跳数据包，如果发3次还没响应，那么断开连接。我们也可以在服务端的<code>/etc/ssh/sshd_config</code>配置文件中添加<code>ClientAliveInterval 30</code>和<code>ClientAliveCountMax 3</code>参数后，重启sshd，表示由服务端向客户端发送心跳数据包。<code>ExitOnForwardFailure</code>表示ssh转发失败后，关闭连接并退出，这样autossh才能监测到错误并重启ssh连接。</p><p>执行上述命令后，我们只能在<code>jumpServer</code>上登录到<code>prodServer</code>。如果我们想要从任意机器上远程登录到<code>prodServer</code>，需要在<code>jumpServer</code>的<code>/etc/ssh/sshd_config</code>中添加<code>GatewayPorts yes</code>参数并重启sshd，之后执行：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -p 22022 prod@jumpServer</span><br></pre></td></tr></table></figure><p>如果<code>prodServer</code>重启后，我们希望<code>autossh</code>也能随系统启动，此时需要将<code>autossh</code>添加到启动项中，以下是以systemd为例，新建一个<code>/lib/systemd/system/autossh.service</code>配置文件来添加autossh启动项：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=autossh</span><br><span class="line">Wants=network-online.target sshd.service</span><br><span class="line">After=network-online.target sshd.service</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">User=geoeye</span><br><span class="line">Environment=&quot;AUTOSSH_GATETIME=0&quot;</span><br><span class="line">ExecStart=/usr/bin/autossh -M 0 -o &quot;ServerAliveInterval=30&quot; -o &quot;ServerAliveCountMax=3&quot; -NR 22022:localhost:22 jingsam@www.foxgis.com</span><br><span class="line">ExecStop=/bin/kill $MAINPID</span><br><span class="line">Restart=on-failure</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>注意上面启动autossh的命令中，要把<code>-f</code>选项去掉，并且autossh使用绝对路径。</p><p>最后执行以下命令执行来启动autossh：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl <span class="built_in">enable</span> autossh</span><br><span class="line">sudo systemctl start autossh</span><br></pre></td></tr></table></figure><h2 id="免密登录"><a href="#免密登录" class="headerlink" title="免密登录"></a>免密登录</h2><p>不管是<code>prodServer</code>在系统启动时反向连接到<code>jumpServer</code>，还是<code>testServer</code>完成自动测试后推送部署包到<code>prodServer</code>，这些过程都是后台自动执行，我们没有机会去输入登录密码。因此，我们需要在三者之间配置ssh key，实现免密登录。</p><p>首先是<code>prodServer</code>连接到<code>jumpServer</code>，这一步比较简单，只需在<code>prodServer</code>使用<code>ssh-copy-id</code>命令将公钥复制到<code>jumpServer</code>即可。</p><p>然后是<code>testServer</code>连接到<code>prodServer</code>，这一步稍微有点复杂，主要是因为<code>testServer</code>一般是动态创建的虚拟机或容器，测试完就删除了，所以没办法提前将<code>testServer</code>的公钥复制到<code>prodServer</code>。解决的思路是使用任意机器登录一次<code>prodServer</code>并将改机器的公钥复制到<code>prodServer</code>，然后将该该机器的私钥复制到<code>testServer</code>，让<code>testServer</code>伪装成为该机器登录<code>prodServer</code>。Gitlab、Travis-ci、Circle CI各自有不同的方法安全地传输ssh key，具体参考相应的文档。Gitlab可以在project和group中的“设置-&gt;CI/CD-&gt;Variables”中设置环境变量，如下图：</p><p><img src="/assets/gitlab-variables.png" alt=""></p><p>设置好之后，配置<code>.gitlab-ci.yml</code>：</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line"><span class="attr">  stage:</span> <span class="string">deploy</span></span><br><span class="line"><span class="attr">  before_script:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">'which ssh-agent || ( apt-get update -y &amp;&amp; apt-get install openssh-client -y )'</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">eval</span> <span class="string">$(ssh-agent</span> <span class="bullet">-s)</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">echo</span> <span class="string">"$SSH_PRIVATE_KEY"</span> <span class="string">| tr -d '\r' | ssh-add - &gt; /dev/null</span></span><br><span class="line"><span class="string">    - mkdir -p ~/.ssh</span></span><br><span class="line"><span class="string">    - chmod 700 ~/.ssh</span></span><br><span class="line"><span class="string">    - ssh-keyscan www.foxgis.com &gt;&gt; ~/.ssh/known_hosts</span></span><br><span class="line"><span class="string">    - chmod 644 ~/.ssh/known_hosts</span></span><br><span class="line"><span class="string">    - '[[ -f /.dockerenv ]] &amp;&amp; echo -e "Host *\n\tStrictHostKeyChecking no\n\n" &gt; ~/.ssh/config'</span></span><br><span class="line"><span class="string"></span><span class="attr">  script:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">ssh</span> <span class="bullet">-p</span> <span class="number">22022</span> <span class="string">prod@jumpServer</span> <span class="comment"># 这里写具体的部署逻辑</span></span><br><span class="line"><span class="attr">  only:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">master</span></span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文借助ssh反向代理技术，实现了内网服务器与测试服务器的互通，并且利用autossh解决了ssh持久连接的问题，以及利用复制私钥来实现测试服务器到内网服务器的免密登录。最终，结合这些技术，实现了内网服务的自动化部署。</p><p>在实际测试过程中，ssh的反向代理连接速度慢而且不是很稳定，下一步研究webhook技术来实现自动部署。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;当前持续集成的流行，大大提高了开发迭代的速度。自动化部署作为持续集成的最后一公里，对于完成整个闭环至关重要。如果生产服务器部署至公网且开通了ssh端口，那么测试服务器完成测试打包后，可以很方便地利用ssh将部署包推送到生产服务器上完成部署。但是，如果生产服务器部署在内网，测
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>简单搞定Nginx日志分割</title>
    <link href="https://jingsam.github.io/2019/01/15/nginx-access-log.html"/>
    <id>https://jingsam.github.io/2019/01/15/nginx-access-log.html</id>
    <published>2019-01-15T10:32:52.000Z</published>
    <updated>2019-01-17T10:24:43.672Z</updated>
    
    <content type="html"><![CDATA[<p>nginx日志分割是很常见的运维工作，关于这方面的文章也很多，通常无外乎两种做法：一是采用cron定期执行shell脚本对日志文件进行归档；二是使用专门日志归档工作logrotate。</p><p>第一种写shell脚本的方法用得不多，毕竟太原始。相比之下，使用logrotate则要省心得多，配置logrotate很简单。关于如何配置logrotate不是本文要讲的内容，感兴趣的话可以自行搜索。</p><p>虽然大多数Linux发行版都自带了logrotate，但在有些情况下不见得安装了logrotate，比如nginx的docker镜像、较老版本的Linux发行版。虽然我们可以使用包管理器安装logrotate，但前提是服务器能够访问互联网，企业内部的服务器可不一定能够联网。</p><p>其实我们有更简单的方法，从nginx 0.7.6版本开始<code>access_log</code>的路径配置可以包含变量，我们可以利用这个特性来实现日志分割。例如，我们想按天来分割日志，那么我们可以这样配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">access_log logs/access-$logdate.log main;</span><br></pre></td></tr></table></figure><p>那么接下来的问题是我们怎么提取出<code>$logdate</code>这个变量？网上有建议使用下面的方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">if ($time_iso8601 ~ &quot;^(\d&#123;4&#125;)-(\d&#123;2&#125;)-(\d&#123;2&#125;)&quot;) &#123;</span><br><span class="line">  set $year $1;</span><br><span class="line">  set $month $2;</span><br><span class="line">  set $day $3;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">access_log logs/access-$year-$month-$day.log main;</span><br></pre></td></tr></table></figure><p>上面的方法有两个问题：一是如果<code>if</code>条件不成立，那么<code>$year</code>、<code>$month</code>和<code>$month</code>这三个变量将不会被设置，那么日志将会记录到<code>access-$year-$month-$day.log</code>这个文件中；二是<code>if</code>只能出现在<code>server</code>和<code>location</code>块中，而<code>access_log</code>通常会配置到顶层的<code>http</code>块中，这时候<code>if</code>就不适用。</p><p>如果要在<code>http</code>块中设置<code>access_log</code>，更好的方法是使用<code>map</code>指令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">map $time_iso8601 $logdate &#123;</span><br><span class="line">  &apos;~^(?&lt;ymd&gt;\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;)&apos; $ymd;</span><br><span class="line">  default                       &apos;date-not-found&apos;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">access_log logs/access-$logdate.log main;</span><br></pre></td></tr></table></figure><p><code>map</code>指令通过设置默认值，保证<code>$logdate</code>始终有值，并且可以出现在<code>http</code>块中，完美地解决了<code>if</code>指令的问题。</p><p>最后，为了提高日志的效率，建议配置<code>open_log_file_cache</code>，完整的日志分割配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">log_format  main  &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos;</span><br><span class="line">                  &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos;</span><br><span class="line">                  &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;;</span><br><span class="line"></span><br><span class="line">map $time_iso8601 $logdate &#123;</span><br><span class="line">  &apos;~^(?&lt;ymd&gt;\d&#123;4&#125;-\d&#123;2&#125;-\d&#123;2&#125;)&apos; $ymd;</span><br><span class="line">  default                       &apos;date-not-found&apos;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">access_log logs/access-$logdate.log main;</span><br><span class="line">open_log_file_cache max=10;</span><br></pre></td></tr></table></figure><p>本文完。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;nginx日志分割是很常见的运维工作，关于这方面的文章也很多，通常无外乎两种做法：一是采用cron定期执行shell脚本对日志文件进行归档；二是使用专门日志归档工作logrotate。&lt;/p&gt;
&lt;p&gt;第一种写shell脚本的方法用得不多，毕竟太原始。相比之下，使用logro
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>离线安装npm包的几种方法</title>
    <link href="https://jingsam.github.io/2018/11/24/npm-package-offline-install.html"/>
    <id>https://jingsam.github.io/2018/11/24/npm-package-offline-install.html</id>
    <published>2018-11-24T08:56:19.000Z</published>
    <updated>2018-11-25T08:31:19.164Z</updated>
    
    <content type="html"><![CDATA[<p>这段时间的工作主题就是Linux<br>下的“离线部署”，包括mongo、mysql、postgresql、nodejs、nginx等软件的离线部署。平常在服务器上借助apt-get就能轻松搞定的事情，在离线环境下就变得异常艰难。<a href="1">上一篇文章</a>讲了使用snap离线安装软件的方式，但对于npm包怎么离线部署，snap是无能为力的。本篇文章就来讲一讲离线安装npm包的几种方法。</p><p>接下来的部分，我将以离线安装pm2为例来进行说明。pm2是一个进程守护程序，用于启动node集群和服务进程出错时自动重启，在生产环境下部署nodejs应用一般都会使用到。</p><h1 id="使用npm-link"><a href="#使用npm-link" class="headerlink" title="使用npm link"></a>使用<code>npm link</code></h1><p>使用<code>npm link</code>的方式是最常用的方法，具体做法是在联网机器上下载pm2的源码并安装好依赖，拷贝到离线服务器上，最后借助<code>npm link</code>将pm2链接到全局区域。</p><p>首先，将pm2的源代码克隆下来：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https://github.com/Unitech/pm2.git</span><br></pre></td></tr></table></figure><p>然后进入到pm2项目中，安装好所有的依赖：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd pm2</span><br><span class="line">$ npm install</span><br></pre></td></tr></table></figure><p>将安装好依赖的pm2文件夹拷贝到目标服务器上，进入pm2目录链接到全局区域：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cd pm2</span><br><span class="line">$ npm link</span><br></pre></td></tr></table></figure><p>这种方式最关键的是借助<code>npm link</code>完成链接，但<code>npm link</code>这条命令本意是设计给开发人员调试用的。但开发人员开发某个全局命令工具的时候，通过将命令从本地工程目录链接到全局，这样调试的时候，可以实时查看本地代码在全局环境下的执行情况。所以，<code>npm link</code>的项目需要安装所有的依赖，包括<code>dependencies</code>以及<code>devDependencies</code>，而我们如果只是使用而不是开发某个包的话，正常情况下不应该安装<code>devDependencies</code>。</p><p>总而言之，这种方式优点是比较简单，缺点是安装了不需要的<code>devDependencies</code>，对于有“洁癖”的人是难以忍受的。</p><h1 id="使用npm-bundle"><a href="#使用npm-bundle" class="headerlink" title="使用npm bundle"></a>使用<code>npm bundle</code></h1><p>那有什么方法相比于上一种方法更干净呢？答案是使用npm-bundle工具将pm2的所有依赖打包，然后到目标服务器上使用<code>npm install &lt;tarball file&gt;</code>安装。</p><p>首先在联网机器上安装npm-bundle工具：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g npm-bundle</span><br></pre></td></tr></table></figure><p>然后打包pm2：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm-bundle pm2</span><br></pre></td></tr></table></figure><p>上面的命令会生成一个tgz的包文件，复制到目标服务器上安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ npm install -g ./pm2-3.2.2.tgz</span><br></pre></td></tr></table></figure><p>npm-bundle的本质是借助<code>npm pack</code>来实现打包的。<code>npm pack</code>会打包包本身以及<code>bundledDependencies</code>中的依赖，npm-bundle则是将pm2的所有<code>dependencies</code>记录到<code>bundledDependencies</code>，来实现所有依赖的打包。</p><p>这种方式不需要安装多余的<code>devDependencies</code>，并且不需要克隆pm2的源码，比第一种方法更方便。</p><p>更新：<code>npm-bundle</code>对于scoped packages的处理有bug，不能正确地打包，这时考虑采用第一种方式。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这段时间的工作主题就是Linux&lt;br&gt;下的“离线部署”，包括mongo、mysql、postgresql、nodejs、nginx等软件的离线部署。平常在服务器上借助apt-get就能轻松搞定的事情，在离线环境下就变得异常艰难。&lt;a href=&quot;1&quot;&gt;上一篇文章&lt;/a&gt;讲
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Ubuntu离线部署snap软件包</title>
    <link href="https://jingsam.github.io/2018/11/22/install-snaps-offline.html"/>
    <id>https://jingsam.github.io/2018/11/22/install-snaps-offline.html</id>
    <published>2018-11-22T13:03:07.000Z</published>
    <updated>2018-11-22T14:52:40.388Z</updated>
    
    <content type="html"><![CDATA[<p>Ubuntu借助包管理器apt-get安装软件包很方便，前提是服务器要能够联网。政府或企业内网的服务器，通常是不与互联网连通的，这时候部署软件只能借助文件拷贝的方式，感觉回到了原始时代。更大的问题是，要部署的软件包需要先安装很多依赖，依赖自己可能还有依赖，并且各种依赖还有版本要求。如果通过手动下载各个依赖包的方法部署，对于复杂的软件，变成了不可能的任务。</p><p>在离线部署方面，Windows明显比Linux做得好，Windows软件包通常会将软件所需的依赖打包，部署时只需拷贝一个软件安装包即可。那Linux有没有类似Windows软件安装包的东西呢？幸运的是，Ubuntu提供了snap软件包机制，可以用来简化离线部署。</p><p>snap软件包类似于windows的软件安装包，将所需的依赖都统一打包到软件包中，部署时只需拷贝snap文件。另外，snap也加强了安全隔离机制，通过注册软件包的签名和权限控制信息，使得snap软件运行在“沙盒”环境中。</p><p>从Ubuntu 16.04起，snap环境是自带的，可以直接使用。如果是早于16.04的版本且服务器不能联网，安装snap环境很困难，你只能自求多福了。下面以安装docker为例，来说明离线安装snap包的方法。</p><h1 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h1><p>首先，我们需要在能联网的机器上将相关的snap包下载下来。不仅要下载docker软件包，还需要下载core软件包。core软件包是snap的核心运行时，几乎所有的snap包都依赖core运行时，Ubuntu 16.04自带了snap环境却没安装core运行时，实在是让人有些搞不懂。</p><p>下载有snap包两种方式。一种方法是在能联网的Ubuntu上使用<code>snap download</code>命令下载：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ snap download core</span><br><span class="line">Fetching snap &quot;core&quot;</span><br><span class="line">Fetching assertions for &quot;core&quot;</span><br><span class="line">Install the snap with:</span><br><span class="line">   snap ack core_5897.assert</span><br><span class="line">   snap install core_5897.snap</span><br><span class="line">$ snap download docker</span><br><span class="line">Fetching snap &quot;docker&quot;</span><br><span class="line">Fetching assertions for &quot;docker&quot;</span><br><span class="line">Install the snap with:</span><br><span class="line">   snap ack docker_321.assert</span><br><span class="line">   snap install docker_321.snap</span><br></pre></td></tr></table></figure><p>以上命令将会得到<code>.assert</code>和<code>.snap</code>两类文件，其中<code>.assert</code>是软件包的元数据信息，包括签名和权限控制信息，<code>.snap</code>是实际的安装文件。</p><p>另外一种方法是到<a href="https://uappexplorer.com/snaps" target="_blank" rel="noopener">uApp Explorer</a>网站上下载，好处是不需要有Ubuntu环境，缺点是只能下载<code>.snap</code>文件，无法下载<code>.assert</code>文件。</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>安装snap包的方法很简单：将软件包拷贝到服务器上，安装时首先注册<code>.assert</code>，然后再安装<code>.assert</code>。对于首次安装，需要安装core和docker两个软件包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo snap ack core_5897.assert</span><br><span class="line">$ sudo snap install core_5897.snap</span><br><span class="line">$ sudo snap ack docker_321.assert</span><br><span class="line">$ sudo snap install docker_321.snap</span><br></pre></td></tr></table></figure><p>这样就完成了docker的安装。</p><p>要是我们是从<a href="https://uappexplorer.com/snaps" target="_blank" rel="noopener">uApp Explorer</a>网站上下载的软件包，缺少<code>.assert</code>文件怎么办？其实我们可以使用<code>snap install --dangerous</code>方式安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo snap install core.snap --dangerous</span><br><span class="line">$ sudo snap install docker.snap --dangerous</span><br></pre></td></tr></table></figure><p>如<code>--dangerous</code>所提示的是，这种模式有些“危险”。这是因为缺少<code>.assert</code>文件所描述的签名信息和权限控制信息，意味着软件不是在“沙盒”环境下执行的，运行过程不受控。其实，大可不必紧张，只要snap包来源可信，一般没什么问题，毕竟以前咱们没有snap包的时候不都是这么干的么。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Ubuntu借助包管理器apt-get安装软件包很方便，前提是服务器要能够联网。政府或企业内网的服务器，通常是不与互联网连通的，这时候部署软件只能借助文件拷贝的方式，感觉回到了原始时代。更大的问题是，要部署的软件包需要先安装很多依赖，依赖自己可能还有依赖，并且各种依赖还有版
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Docker容器访问宿主机网络</title>
    <link href="https://jingsam.github.io/2018/10/16/host-in-docker.html"/>
    <id>https://jingsam.github.io/2018/10/16/host-in-docker.html</id>
    <published>2018-10-16T02:26:26.000Z</published>
    <updated>2018-10-18T16:04:03.396Z</updated>
    
    <content type="html"><![CDATA[<h1 id="缘起"><a href="#缘起" class="headerlink" title="缘起"></a>缘起</h1><p>最近部署一套系统，使用nginx作反向代理，其中nginx是使用docker方式运行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -d --name nginx $PWD:/etc/nginx -p 80:80 -p 443:443 nginx:1.15</span><br></pre></td></tr></table></figure><p>需要代理的API服务运行在宿主机的<code>1234</code>端口，<code>nginx.conf</code>相关配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  location /api &#123;</span><br><span class="line">    proxy_pass http://localhost:1234</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>结果访问的时候发现老是报<code>502 Bad Gateway</code>错误，错误日志显示无法连接到upstream。</p><p>仔细想一想，<code>nginx.conf</code>中的<code>localhost</code>似乎有问题。由于nginx是运行在docker容器中的，这个<code>localhost</code>是容器的localhost，而不是宿主机的localhost。</p><p>到这里，就出现了本文要解决的问题：如何从容器中访问到宿主机的网络？通过搜索网络，有如下几种方法：</p><h1 id="使用宿主机IP"><a href="#使用宿主机IP" class="headerlink" title="使用宿主机IP"></a>使用宿主机IP</h1><p>在安装Docker的时候，会在宿主机安装一个虚拟网关<code>docker0</code>，我们可以使用宿主机在<code>docker0</code>上的IP地址来代替<code>localhost</code>。</p><p>首先，使用如下命令查询宿主机IP地址：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ ip addr show docker0</span><br><span class="line">3: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 02:42:d5:4c:f2:1e brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.1/16 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::42:d5ff:fe4c:f21e/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>可以发现宿主机的IP是<code>172.17.0.1</code>，那么将<code>proxy_pass http://localhost:1234</code>改为<code>proxy_pass http://172.17.0.1:1234</code>就可以解决<code>502 Bad Gateway</code>错误。</p><p>但是，不同系统下宿主机的IP是不同的，例如Linux下一般是<code>172.17.0.1</code>, macOS下一般是<code>192.168.65.1</code>，并且这个IP还可以更改。所以使用IP配置<code>nginx.conf</code>，不能跨环境通用。</p><h1 id="使用host网络"><a href="#使用host网络" class="headerlink" title="使用host网络"></a>使用host网络</h1><p>Docker容器运行的时候有<code>host</code>、<code>bridge</code>、<code>none</code>三种网络可供配置。默认是<code>bridge</code>，即桥接网络，以桥接模式连接到宿主机；<code>host</code>是宿主网络，即与宿主机共用网络；<code>none</code>则表示无网络，容器将无法联网。</p><p>当容器使用<code>host</code>网络时，容器与宿主共用网络，这样就能在容器中访问宿主机网络，那么容器的<code>localhost</code>就是宿主机的<code>localhost</code>。</p><p>在docker中使用<code>--network host</code>来为容器配置<code>host</code>网络：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -d --name nginx --network host nginx</span><br></pre></td></tr></table></figure><p>上面的命令中，没有必要像前面一样使用<code>-p 80:80 -p 443:443</code>来映射端口，是因为本身与宿主机共用了网络，容器中暴露端口等同于宿主机暴露端口。</p><p>使用host网络不需要修改<code>nginx.conf</code>，仍然可以使用<code>localhost</code>，因而通用性比上一种方法好。但是，由于<code>host</code>网络没有<code>bridge</code>网络的隔离性好，使用<code>host</code>网络安全性不如<code>bridge</code>高。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文提出了使用宿主机IP和使用host网络两种方法，来实现从容器中访问宿主机的网络。两种方法各有优劣，使用宿主机IP隔离性更好，但通用性不好；使用host网络，通用性好，但带来了暴露宿主网络的风险。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;缘起&quot;&gt;&lt;a href=&quot;#缘起&quot; class=&quot;headerlink&quot; title=&quot;缘起&quot;&gt;&lt;/a&gt;缘起&lt;/h1&gt;&lt;p&gt;最近部署一套系统，使用nginx作反向代理，其中nginx是使用docker方式运行：&lt;/p&gt;
&lt;figure class=&quot;highlig
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>手动挡模式申请Let&#39;s Encrypt通配符证书</title>
    <link href="https://jingsam.github.io/2018/10/12/lets-encrypt.html"/>
    <id>https://jingsam.github.io/2018/10/12/lets-encrypt.html</id>
    <published>2018-10-12T04:00:00.000Z</published>
    <updated>2018-12-23T07:27:10.752Z</updated>
    
    <content type="html"><![CDATA[<p>本篇文章要讲的内容是申请Let’s Encrypt通配符证书，但是标题中加一个“手动挡”模式是什么意思呢？我们拿学车为例，当我们学会了开手动挡，开自动挡自然不在话下。同理，如果我们弄明白了手动申请Let’s Encrypt证书的步骤，以后使用自动化工具自然也是手到擒来。</p><p>网上关于申请Let’s Encrypt证书的文章多如牛毛，本篇文章的不同在于以下几点：</p><ol><li>使用手动模式申请证书；</li><li>申请的是通配符证书，一个证书适用多个域名；</li><li>使用Docker来完成申请操作，避免安装一大堆软件污染系统；</li><li>可以在本地电脑上完成申请操作，更灵活。</li></ol><h1 id="验证域名的三种模式"><a href="#验证域名的三种模式" class="headerlink" title="验证域名的三种模式"></a>验证域名的三种模式</h1><p>申请证书要解决的一个关键问题是：如何证明域名是你所拥有的？Let’s Encrypt提供了三种模式：http、dns、tls-sni。</p><p>http模式就是Let’s Encrypt给你一个随机字符串，你需要在Web服务器的<code>/.well-known/acme-challenge/</code>服务路径下放置一个以该字符串命名的文件，当Let’s Encrypt能够访问到这个文件时，证明你是这个域名的所有者。</p><p>dns模式同样是Let’s Encrypt给你一个随机字符串，你需要以该字符串建立一个DNS TXT记录，当Let’s Encrypt查询域名的TXT记录时，发现得到的字符串一致，则证明你是这个域名的所有者。</p><p>tls-sni模式没有仔细研究过，似乎通过SSL加密传输。http走的是80端口，tls-sni走的是443端口。由于当前的tls-sni似乎有漏洞，该模式一度遭禁用，所以不推荐采用此模式，本文也不加以讨论。</p><p>http模式和dns模式各有优劣，适用于不同的场景。http模式不需要操作DNS记录，只需要新建一个文件就可以完成验证，带来的限制就是验证过程必须操作服务器；dns模式不需要操作服务器，只需添加DNS TXT记录就行，缺点是必须登录到域名提供商的页面上修改DNS记录。不管是http模式还是dns模式，申请证书的操作都是可以在任意电脑上完成，申请完之后再将证书复制到服务器上。http模式验证过程需要操作服务器，dns模式验证过程需要操作DNS。</p><p>由于我们要申请的是通配符证书，必须使用dns模式验证，为什么呢？以通配符域名<code>*.example.com</code>为例，它包含的域名千千万万，不太可能使用http模式去验证每个域名。dns模式的验证能力更强，如果用户具有操作<code>example.com</code>域名的DNS记录权限，那当然是拥有通配符域名<code>*.example.com</code>。</p><p>搞清楚申请域名关键问题，接下来说明申请域名的步骤。</p><h1 id="申请域名的步骤"><a href="#申请域名的步骤" class="headerlink" title="申请域名的步骤"></a>申请域名的步骤</h1><p>申请let’s encrypt的客户端软件很多，这里采用的是官方推荐的客户端certbot。运行certbot需要python环境，还需要在系统全局环境安装一些python包。考虑到证书的有效期是90天，申请证书并不是一个频繁的操作，我不想因为一个低频使用的软件污染我的全局系统，因此采用Docker作为运行环境。</p><p>申请let’s encrypt只需要一条命令，之后certbot通过交互的方式询问申请信息，很人性化：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -it --rm --name certbot \</span><br><span class="line">    -v $PWD:/etc/letsencrypt \</span><br><span class="line">    certbot/certbot:v0.27.1 \</span><br><span class="line">    certonly --manual --preferred-challenges=dns-01 \</span><br><span class="line">    --server=https://acme-v02.api.letsencrypt.org/directory</span><br></pre></td></tr></table></figure><p>上述命令中，<code>-v $PWD:/etc/letsencrypt</code>表示把当前文件夹映射到docker中的<code>/etc/letsencrypt</code>文件夹，这样certbot生成的证书将出现在当前文件夹中；<code>certonly</code>表示证书申请子命令，还有<code>renew</code>、<code>revoke</code>、<code>delete</code>等其他子命令；<code>--manual</code>是手动申请模式；<code>--preferred-challenges=dns-01</code>表示采用dns模式验证，默认采用http模式验证；<code>--server=https://acme-v02.api.letsencrypt.org/directory</code>表示指向ACME V2版本服务器，默认指向ACME V1版本服务器，但只有ACME V2才支持通配符证书。</p><p>接着需要提供一个email地址用于接收证书更新和安全问题提醒：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Saving debug log to /var/log/letsencrypt/letsencrypt.log</span><br><span class="line">Plugins selected: Authenticator manual, Installer None</span><br><span class="line">Enter email address (used for urgent renewal and security notices) (Enter &apos;c&apos; to</span><br><span class="line">cancel): XXX@XX.com</span><br></pre></td></tr></table></figure><p>需要同意用户协议：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -</span><br><span class="line">Please read the Terms of Service at</span><br><span class="line">https://letsencrypt.org/documents/LE-SA-v1.2-November-15-2017.pdf. You must</span><br><span class="line">agree in order to register with the ACME server at</span><br><span class="line">https://acme-v02.api.letsencrypt.org/directory</span><br><span class="line">- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -</span><br><span class="line">(A)gree/(C)ancel: A</span><br></pre></td></tr></table></figure><p>询问你需不需要接收电子前哨基金会（EFF）的新闻、活动，可以选择不接收：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -</span><br><span class="line">Would you be willing to share your email address with the Electronic Frontier</span><br><span class="line">Foundation, a founding partner of the Let&apos;s Encrypt project and the non-profit</span><br><span class="line">organization that develops Certbot? We&apos;d like to send you email about our work</span><br><span class="line">encrypting the web, EFF news, campaigns, and ways to support digital freedom.</span><br><span class="line">- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -</span><br><span class="line">(Y)es/(N)o: N</span><br></pre></td></tr></table></figure><p>接下来就需要填写要申请证书的域名，这里需要注意一点的是<code>*.example.com</code>并不包括裸域名<code>example.com</code>，如果申请的证书需要囊括<code>example.com</code>，就必须要同时包含<code>example.com</code>和<code>*.example.com</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Please enter in your domain name(s) (comma and/or space separated)  (Enter &apos;c&apos;</span><br><span class="line">to cancel): example.com, *.example.com</span><br><span class="line">Obtaining a new certificate</span><br><span class="line">Performing the following challenges:</span><br><span class="line">dns-01 challenge for example.com</span><br><span class="line">dns-01 challenge for example.com</span><br></pre></td></tr></table></figure><p>申请证书的电脑的IP会被记录，可能是防治滥用吧，需要同意记录IP：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -</span><br><span class="line">NOTE: The IP of this machine will be publicly logged as having requested this</span><br><span class="line">certificate. If you&apos;re running certbot in manual mode on a machine that is not</span><br><span class="line">your server, please ensure you&apos;re okay with that.</span><br><span class="line"></span><br><span class="line">Are you OK with your IP being logged?</span><br><span class="line">- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -</span><br><span class="line">(Y)es/(N)o: Y</span><br></pre></td></tr></table></figure><p>然后开始使用dns模式验证域名，这时候需要登录到你的域名服务商的DNS编辑页面上，在<code>_acme-challenge.example.com</code>增加两个TXT记录，一个用来验证<code>example.com</code>，另一个用来验证<code>*.example.com</code>。TXT记录的内容就是命令行中提供的随机字符串：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -</span><br><span class="line">Please deploy a DNS TXT record under the name</span><br><span class="line">_acme-challenge.example.com with the following value:</span><br><span class="line"></span><br><span class="line">FvMdm......6scC4</span><br><span class="line"></span><br><span class="line">Before continuing, verify the record is deployed.</span><br><span class="line">- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -</span><br><span class="line">Press Enter to Continue</span><br><span class="line"></span><br><span class="line">- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -</span><br><span class="line">Please deploy a DNS TXT record under the name</span><br><span class="line">_acme-challenge.example.com with the following value:</span><br><span class="line"></span><br><span class="line">H1UHs......Lc90en4dY</span><br><span class="line"></span><br><span class="line">Before continuing, verify the record is deployed.</span><br><span class="line">- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -</span><br></pre></td></tr></table></figure><p>TXT记录修改好之后，使用<code>dig -t TXT _acme-challenge.example.com</code>来验证TXT记录是否生效了。如果确认生效了，则按回车完成证书的申请：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Press Enter to Continue</span><br><span class="line">Waiting for verification...</span><br><span class="line">Cleaning up challenges</span><br><span class="line"></span><br><span class="line">IMPORTANT NOTES:</span><br><span class="line"> - Congratulations! Your certificate and chain have been saved at:</span><br><span class="line">   /etc/letsencrypt/live/example.com/fullchain.pem</span><br><span class="line">   Your key file has been saved at:</span><br><span class="line">   /etc/letsencrypt/live/example.com/privkey.pem</span><br><span class="line">   Your cert will expire on 2019-01-10. To obtain a new or tweaked</span><br><span class="line">   version of this certificate in the future, simply run certbot</span><br><span class="line">   again. To non-interactively renew *all* of your certificates, run</span><br><span class="line">   &quot;certbot renew&quot;</span><br><span class="line"> - Your account credentials have been saved in your Certbot</span><br><span class="line">   configuration directory at /etc/letsencrypt. You should make a</span><br><span class="line">   secure backup of this folder now. This configuration directory will</span><br><span class="line">   also contain certificates and private keys obtained by Certbot so</span><br><span class="line">   making regular backups of this folder is ideal.</span><br><span class="line"> - If you like Certbot, please consider supporting our work by:</span><br><span class="line"></span><br><span class="line">   Donating to ISRG / Let&apos;s Encrypt:   https://letsencrypt.org/donate</span><br><span class="line">   Donating to EFF:                    https://eff.org/donate-le</span><br></pre></td></tr></table></figure><p>上面的信息告诉我们，证书保存在<code>/etc/letsencrypt/live/example.com/fullchain.pem</code>，证书的秘钥保存在<code>/etc/letsencrypt/live/example.com/privkey.pem</code>，我们可以把这两个文件拷贝到服务器上用于设置https。</p><h1 id="证书更新"><a href="#证书更新" class="headerlink" title="证书更新"></a>证书更新</h1><p>证书的有效期是90天，在证书失效前30天之内，Let’s Encrypt会发邮件通知我们。</p><p>虽然有专门的<code>renew</code>命令用来自动更新证书，但是我们申请的是通配符证书，仍然需要手动验证域名。因此，我们需要重新运行原来申请证书的命令来更新证书：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -it --rm --name certbot \</span><br><span class="line">    -v $PWD:/etc/letsencrypt \</span><br><span class="line">    certbot/certbot:v0.27.1 \</span><br><span class="line">    certonly --manual --preferred-challenges=dns-01 \</span><br><span class="line">    --server=https://acme-v02.api.letsencrypt.org/directory</span><br></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文通过手动模式一步一步操作，完成了通配符域名的申请。只要理解了let’s encrypt验证域名的http模式和dns模式的原理，申请域名就能做到心中有数。下一篇将讲一讲在nginx环境下，如何使用申请到的证书配置https。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本篇文章要讲的内容是申请Let’s Encrypt通配符证书，但是标题中加一个“手动挡”模式是什么意思呢？我们拿学车为例，当我们学会了开手动挡，开自动挡自然不在话下。同理，如果我们弄明白了手动申请Let’s Encrypt证书的步骤，以后使用自动化工具自然也是手到擒来。&lt;/
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Git内部原理之Git引用</title>
    <link href="https://jingsam.github.io/2018/10/12/git-reference.html"/>
    <id>https://jingsam.github.io/2018/10/12/git-reference.html</id>
    <published>2018-10-12T03:48:03.000Z</published>
    <updated>2018-10-12T03:48:03.794Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章本应该在6月份就完成，拖了4个月之后，终于鼓起勇气捡起来，实在惭愧。坚持写文章就像长跑，途中跑起来基本是靠惯性，如果停下来再起跑就很累很困难。</p><p>闲话不多说，本篇继续承接前文讲一讲Git内部原理，本篇的主题是Git引用的原理。</p><p>首先来搞清楚什么是Git引用，前文讲了Git提交对象的哈希、存储原理，理论上我们只要知道该对象的hash值，就能往前推出整个提交历史，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git log --pretty=oneline 3ac728ac62f0a7b5ac201fd3ed1f69165df8be31</span><br><span class="line">3ac728ac62f0a7b5ac201fd3ed1f69165df8be31 third commit</span><br><span class="line">d4d2c6cffb408d978cb6f1eb6cfc70e977378a5c second commit</span><br><span class="line">db1d6f137952f2b24e3c85724ebd7528587a067a first commit</span><br></pre></td></tr></table></figure><p>现在问题来了，提交对象的这40位hash值不好记忆，Git引用相当于给40位hash值取一个别名，便于识别和读取。Git引用对象都存储在<code>.git/refs</code>目录下，该目录下有3个子文件夹<code>heads</code>、<code>tags</code>和<code>remotes</code>，分别对应于HEAD引用、标签引用和远程引用，下面分别讲一讲每种引用的原理。</p><h1 id="HEAD引用"><a href="#HEAD引用" class="headerlink" title="HEAD引用"></a>HEAD引用</h1><p>HEAD引用是用来指向每个分支的最后一次提交对象，这样切换到一个分支之后，才能知道分支的“尾巴”在哪里。HEAD引用存储在<code>.git/refs/heads</code>目录下，有多少个分支，就有相应的同名HEAD引用对象。例如代码库里面有<code>master</code>和<code>test</code>两个分支，那么<code>.git/refs/heads</code>目录下就存在<code>master</code>和<code>test</code>两个文件，分别记录了分支的最后一次提交。</p><p>HEAD引用的内容就是提交对象的hash值，理论上我们可以手动地构造一个HEAD引用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ echo &quot;3ac728ac62f0a7b5ac201fd3ed1f69165df8be31&quot; &gt; .git/refs/heads/master</span><br></pre></td></tr></table></figure><p>Git提供了一个专有命令<code>update-ref</code>，用来查看和修改Git引用对象，当然也包括HEAD引用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git update-ref refs/heads/master 3ac728ac62f0a7b5ac201fd3ed1f69165df8be31</span><br><span class="line">$ git update-ref refs/heads/master</span><br><span class="line">3ac728ac62f0a7b5ac201fd3ed1f69165df8be31</span><br></pre></td></tr></table></figure><p>上面的命令我们将<code>master</code>分支的HEAD指向了<code>3ac728ac62f0a7b5ac201fd3ed1f69165df8be31</code>，现在用<code>git log</code>查看下<code>master</code>的提交历史，可以发现最后一次提交就是所更新的hash值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git log --pretty=oneline master</span><br><span class="line">3ac728ac62f0a7b5ac201fd3ed1f69165df8be31 (HEAD -&gt; master) third commit</span><br><span class="line">d4d2c6cffb408d978cb6f1eb6cfc70e977378a5c second commit</span><br><span class="line">db1d6f137952f2b24e3c85724ebd7528587a067a first commit</span><br></pre></td></tr></table></figure><p>同理，可以使用同样的方法更新<code>test</code>分支的HEAD：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git update-ref refs/heads/test d4d2c6cffb408d978cb6f1eb6cfc70e977378a5c</span><br><span class="line">$ git log --pretty=oneline test</span><br><span class="line">d4d2c6cffb408d978cb6f1eb6cfc70e977378a5c (test) second commit</span><br><span class="line">db1d6f137952f2b24e3c85724ebd7528587a067a first commit</span><br></pre></td></tr></table></figure><p><code>.git/refs/heads</code>目录下存储了每个分支的HEAD，那怎么知道代码库当前处于哪个分支呢？这就需要一个代码库级别的HEAD引用。<code>.git/HEAD</code>这个文件就是整个代码库级别的HEAD引用。我们先查看一下<code>.git/HEAD</code>文件的内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat .git/HEAD</span><br><span class="line">ref: refs/heads/master</span><br></pre></td></tr></table></figure><p>我们发现<code>.git/HEAD</code>文件的内容不是40位hash值，而像是指向<code>.git/refs/heads/master</code>。尝试切换到<code>test</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout test</span><br><span class="line">$ cat .git/HEAD</span><br><span class="line">ref: refs/heads/test</span><br></pre></td></tr></table></figure><p>切换分支后，<code>.git/HEAD</code>文件的内容也跟着指向<code>.git/refs/heads/test</code>。<code>.git/HEAD</code>也是HEAD引用对象，与一般引用不同的是，它是“符号引用”。符号引用类似于文件的快捷方式，链接到要引用的对象上。</p><p>Git提供专门的命令<code>git symbolic-ref</code>，用来查看和更新符号引用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git symbolic-ref HEAD refs/heads/master</span><br><span class="line">$ git symbolic-ref HEAD refs/heads/test</span><br></pre></td></tr></table></figure><p>至此，我们分析了两种HEAD引用，一种是分支级别的HEAD引用，用来记录各分支的最后一次提交，存储在<code>.git/refs/heads</code>目录下，使用<code>git update-ref</code>来维护；一种是代码库级别的HEAD引用，用来记录代码库所处的分支，存储在<code>.git/HEAD</code>文件，使用<code>git symbolic-ref</code>来维护。</p><h1 id="标签引用"><a href="#标签引用" class="headerlink" title="标签引用"></a>标签引用</h1><p>标签引用，顾名思义就是给Git对象打标签，便于记忆。例如，我们可以将某个提交对象打v1.0标签，表示是1.0版本。标签引用都存储在<code>.git/refs/tags</code>里面。</p><p>标签引用和HEAD引用本质是Git引用对象，同样使用<code>git update-ref</code>来查看和修改：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git update-ref refs/tags/v1.0 d4d2c6cffb408d978cb6f1eb6cfc70e977378a5c</span><br><span class="line">$ cat .git/refs/tags/v1.0</span><br><span class="line">d4d2c6cffb408d978cb6f1eb6cfc70e977378a5c</span><br></pre></td></tr></table></figure><p>还有一种标签引用称为“附注引用”，可以为标签添加说明信息。上面的标签引用打了一个<code>v1.0</code>的标签表示发布1.0版本，有时候发布软件的时候除了版本号信息，还要写更新说明。附注引用就是用来实现打标签的同时，也可以附带说明信息。</p><p>附注引用是怎么实现的呢？与常规标签引用不同的是，它不直接指向提交对象，而是新建一个Git对象存储到<code>.git/objects</code>中，用来记录附注信息，然后附注标签指向这个Git对象。</p><p>使用<code>git tag</code>建立一个附注标签：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git tag -a v1.1 3ac728ac62f0a7b5ac201fd3ed1f69165df8be31 -m &quot;test tag&quot;</span><br><span class="line">$ cat .git/refs/tags/v1.1</span><br><span class="line">8be4d8e4e8e80711dd7bae304ccfa63b35a6eb8c</span><br></pre></td></tr></table></figure><p>使用<code>git cat-file</code>来查看附注标签所指向的Git对象：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ git cat-file -p 8be4d8e4e8e80711dd7bae304ccfa63b35a6eb8c</span><br><span class="line">object 3ac728ac62f0a7b5ac201fd3ed1f69165df8be31</span><br><span class="line">type commit</span><br><span class="line">tag v1.1</span><br><span class="line">tagger jingsam &lt;jing-sam@qq.com&gt; 1529481368 +0800</span><br><span class="line"></span><br><span class="line">test tag</span><br></pre></td></tr></table></figure><p>可以看到，上面的Git对象存储了我们填写的附注信息。</p><p>总之，普通的标签引用和附注引用同样都是存储的是40位hash值，指向一个Git对象，所不同的是普通的标签引用是直接指向提交对象，而附注标签是指向一个附注对象，附注对象再指向具体的提交对象。</p><p>另外，本质上标签引用并不是只可以指向提交对象，实际上可以指向任何Git对象，即可以给任何Git对象打标签。</p><h1 id="远程引用"><a href="#远程引用" class="headerlink" title="远程引用"></a>远程引用</h1><p>远程引用，类似于<code>.git/refs/heads</code>中存储的本地仓库各分支的最后一次提交，在<code>.git/refs/remotes</code>是用来记录多个远程仓库各分支的最后一次提交。</p><p>我们可以使用<code>git remote</code>来管理远程分支：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git remote add origin git@github.com:jingsam/git-test.git</span><br></pre></td></tr></table></figure><p>上面添加了一个<code>origin</code>远程分支，接下来我们把本地仓库的<code>master</code>推送到远程仓库上：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ git push origin master</span><br><span class="line">Counting objects: 9, done.</span><br><span class="line">Delta compression using up to 4 threads.</span><br><span class="line">Compressing objects: 100% (5/5), done.</span><br><span class="line">Writing objects: 100% (9/9), 720 bytes | 360.00 KiB/s, done.</span><br><span class="line">Total 9 (delta 0), reused 0 (delta 0)</span><br><span class="line">To github.com:jingsam/git-test.git</span><br><span class="line"> * [new branch]      master -&gt; master</span><br></pre></td></tr></table></figure><p>这时候在<code>.git/refs/remotes</code>中的远程引用就会更新：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat .git/refs/remotes/origin/master</span><br><span class="line">3ac728ac62f0a7b5ac201fd3ed1f69165df8be31</span><br></pre></td></tr></table></figure><p>和本地仓库的<code>master</code>比较一下，发现是一模一样的，表示远程分支和本地分支是同步的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat .git/refs/heads/master</span><br><span class="line">3ac728ac62f0a7b5ac201fd3ed1f69165df8be31</span><br></pre></td></tr></table></figure><p>由于远程引用也是Git引用对象，所以理论上也可以使用<code>git update-ref</code>来手动维护。但是，我们需要先把代码与远程仓库进行同步，在远程仓库中找到对应分支的HEAD，然后使用<code>git update-ref</code>进行更新，过程比较麻烦。而我们在执行<code>git pull</code>或<code>git push</code>这样的高层命令的时候，远程引用会自动更新。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>到这里，三种Git引用都已分析完毕。总的来说，三种Git引用都统一存储到<code>.git/refs</code>目录下，Git引用中的内容都是40位的hash值，指向某个Git对象，这个对象可以是任意的Git对象，可以是数据对象、树对象、提交对象。三种Git引用都可以使用<code>git update-ref</code>来手动维护。</p><p>三种Git引用对象所不同的是，分别存储于<code>.git/refs/heads</code>、<code>.git/refs/tags</code>、<code>.git/refs/remotes</code>,存储的文件夹不同，赋予了引用对象不同的功能。HEAD引用用来记录本地分支的最后一次提交，标签引用用来给任意Git对象打标签，远程引用正式用来记录远程分支的最后一次提交。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇文章本应该在6月份就完成，拖了4个月之后，终于鼓起勇气捡起来，实在惭愧。坚持写文章就像长跑，途中跑起来基本是靠惯性，如果停下来再起跑就很累很困难。&lt;/p&gt;
&lt;p&gt;闲话不多说，本篇继续承接前文讲一讲Git内部原理，本篇的主题是Git引用的原理。&lt;/p&gt;
&lt;p&gt;首先来搞清楚
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Git内部原理之Git对象存储</title>
    <link href="https://jingsam.github.io/2018/06/15/git-storage.html"/>
    <id>https://jingsam.github.io/2018/06/15/git-storage.html</id>
    <published>2018-06-15T09:50:05.000Z</published>
    <updated>2018-06-19T09:36:55.714Z</updated>
    
    <content type="html"><![CDATA[<p>在<a href="1">Git内部原理之Git对象哈希</a>中，讲解了Git对象hash的原理，接下来的这篇文章讲一讲Git对象如何存储。</p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>数据对象、树对象和提交对象都是存储在<code>.git/objects</code>目录下，目录的结构如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">.git</span><br><span class="line">|-- objects</span><br><span class="line">    |-- 01</span><br><span class="line">    |   |-- 55eb4229851634a0f03eb265b69f5a2d56f341</span><br><span class="line">    |-- 1f</span><br><span class="line">    |   |-- 7a7a472abf3dd9643fd615f6da379c4acb3e3a</span><br><span class="line">    |-- 83</span><br><span class="line">        |-- baae61804e65cc73a7201a7252750c76066a30</span><br></pre></td></tr></table></figure><p>从上面的目录结构可以看出，Git对象的40位hash分为两部分：头两位作为文件夹，后38位作为对象文件名。所以一个Git对象的存储路径规则为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.git/objects/hash[0, 2]/hash[2, 40]</span><br></pre></td></tr></table></figure><p>这里就产生了一个疑问：为什么Git要这么设计目录结构，而不直接用Git对象的40位hash作为文件名？原因是有两点：</p><p>1.有些文件系统对目录下的文件数量有限制。例如，FAT32限制单目录下的最大文件数量是65535个，如果使用U盘拷贝Git文件就可能出现问题。<br>2.有些文件系统访问文件是一个线性查找的过程，目录下的文件越多，访问越慢。</p><p>在在<a href="1">Git内部原理之Git对象哈希</a>中，我们知道Git对象会在原内容前加个一个头部：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">store = header + content</span><br></pre></td></tr></table></figure><p>Git对象在存储前，会使用zlib的deflate算法进行压缩，即简要描述为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zlib_store = zlib.deflate(store)</span><br></pre></td></tr></table></figure><p>压缩后的<code>zlib_store</code>按照Git对象的路径规则存储到<code>.git/objects</code>目录下。</p><p>总结下Git对象存储的算法步骤：</p><p>1.计算<code>content</code>长度，构造<code>header</code>;2.将<code>header</code>添加到<code>content</code>前面，构造Git对象；<br>3.使用sha1算法计算Git对象的40位hash码；<br>4.使用zlib的deflate算法压缩Git对象；<br>5.将压缩后的Git对象存储到<code>.git/objects/hash[0, 2]/hash[2, 40]</code>路径下;</p><h1 id="Nodejs实现"><a href="#Nodejs实现" class="headerlink" title="Nodejs实现"></a>Nodejs实现</h1><p>接下来，我们使用Nodejs来实现<code>git hash-object -w</code>的功能，即计算Git对象的hash值并存储到Git文件系统中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">const fs = require(&apos;fs&apos;)</span><br><span class="line">const crypto = require(&apos;crypto&apos;)</span><br><span class="line">const zlib = require(&apos;zlib&apos;)</span><br><span class="line"></span><br><span class="line">function gitHashObject(content, type) &#123;</span><br><span class="line">  // 构造header</span><br><span class="line">  const header = `$&#123;type&#125; $&#123;Buffer.from(content).length&#125;\0`</span><br><span class="line"></span><br><span class="line">  // 构造Git对象</span><br><span class="line">  const store = Buffer.concat([Buffer.from(header), Buffer.from(content)])</span><br><span class="line"></span><br><span class="line">  // 计算hash</span><br><span class="line">  const sha1 = crypto.createHash(&apos;sha1&apos;)</span><br><span class="line">  sha1.update(store)</span><br><span class="line">  const hash = sha1.digest(&apos;hex&apos;)</span><br><span class="line"></span><br><span class="line">  // 压缩Git对象</span><br><span class="line">  const zlib_store = zlib.deflateSync(store)</span><br><span class="line"></span><br><span class="line">  // 存储Git对象</span><br><span class="line">  fs.mkdirSync(`.git/objects/$&#123;hash.substring(0, 2)&#125;`)</span><br><span class="line">  fs.writeFileSync(`.git/objects/$&#123;hash.substring(0, 2)&#125;/$&#123;hash.substring(2, 40)&#125;`, zlib_store)</span><br><span class="line"></span><br><span class="line">  console.log(hash)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 调用入口</span><br><span class="line">gitHashObject(process.argv[2], process.argv[3])</span><br></pre></td></tr></table></figure><p>最后，测试下能否正确存储Git对象：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ node index.js &apos;hello, world&apos; blob</span><br><span class="line">8c01d89ae06311834ee4b1fab2f0414d35f01102</span><br><span class="line">$ git cat-file -p 8c01d89ae06311834ee4b1fab2f0414d35f01102</span><br><span class="line">hello, world</span><br></pre></td></tr></table></figure><p>由此可见，我们生成了一个合法的Git数据对象，证明算法是正确的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在&lt;a href=&quot;1&quot;&gt;Git内部原理之Git对象哈希&lt;/a&gt;中，讲解了Git对象hash的原理，接下来的这篇文章讲一讲Git对象如何存储。&lt;/p&gt;
&lt;h1 id=&quot;原理&quot;&gt;&lt;a href=&quot;#原理&quot; class=&quot;headerlink&quot; title=&quot;原理&quot;&gt;&lt;/a&gt;原
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Git内部原理之Git对象哈希</title>
    <link href="https://jingsam.github.io/2018/06/10/git-hash.html"/>
    <id>https://jingsam.github.io/2018/06/10/git-hash.html</id>
    <published>2018-06-09T23:25:18.000Z</published>
    <updated>2018-06-15T09:47:53.821Z</updated>
    
    <content type="html"><![CDATA[<p>在上一篇<a href="1">文章</a>中，将了数据对象、树对象和提交对象三种Git对象，每种对象会计算出一个hash值。那么，Git是如何计算出Git对象的hash值？本文的内容就是来解答这个问题。</p><h1 id="Git对象的hash方法"><a href="#Git对象的hash方法" class="headerlink" title="Git对象的hash方法"></a>Git对象的hash方法</h1><p>Git中的数据对象、树对象和提交对象的hash方法原理是一样的，可以描述为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">header = &quot;&lt;type&gt; &quot; + content.length + &quot;\0&quot;</span><br><span class="line">hash = sha1(header + content)</span><br></pre></td></tr></table></figure><p>上面公式表示，Git在计算对象hash时，首先会在对象头部添加一个<code>header</code>。这个<code>header</code>由3部分组成：第一部分表示对象的类型，可以取值<code>blob</code>、<code>tree</code>、<code>commit</code>以分别表示数据对象、树对象、提交对象；第二部分是数据的字节长度；第三部分是一个空字节，用来将<code>header</code>和<code>content</code>分隔开。将<code>header</code>添加到<code>content</code>头部之后，使用<code>sha1</code>算法计算出一个40位的hash值。</p><p>在手动计算Git对象的hash时，有两点需要注意：<br>1.<strong><code>header</code>中第二部分关于数据长度的计算，一定是字节的长度而不是字符串的长度</strong>；2.<strong><code>header + content</code>的操作并不是字符串级别的拼接，而是二进制级别的拼接</strong>。</p><p>各种Git对象的hash方法相同，不同的在于：<br>1.头部类型不同，数据对象是<code>blob</code>，树对象是<code>tree</code>，提交对象是<code>commit</code>；2.数据内容不同，数据对象的内容可以是任意内容，而树对象和提交对象的内容有固定的格式。</p><p>接下来分别讲数据对象、树对象和提交对象的具体的hash方法。</p><h2 id="数据对象"><a href="#数据对象" class="headerlink" title="数据对象"></a>数据对象</h2><p>数据对象的格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">blob &lt;content length&gt;&lt;NULL&gt;&lt;content&gt;</span><br></pre></td></tr></table></figure><p>从上一篇<a href="1">文章</a>中我们知道，使用<code>git hash-object</code>可以计算出一个40位的hash值，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ echo -n &quot;what is up, doc?&quot; | git hash-object --stdin</span><br><span class="line">bd9dbf5aae1a3862dd1526723246b20206e5fc37</span><br></pre></td></tr></table></figure><p>注意，上面在<code>echo</code>后面使用了<code>-n</code>选项，用来阻止自动在字符串末尾添加换行符，否则会导致实际传给<code>git hash-object</code>是<code>what is up, doc?\n</code>，而不是我们直观认为的<code>what is up, doc?</code>。</p><p>为验证前面提到的Git对象hash方法，我们使用<code>openssl sha1</code>来手动计算<code>what is up, doc?</code>的hash值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ echo -n &quot;blob 16\0what is up, doc?&quot; | openssl sha1</span><br><span class="line">bd9dbf5aae1a3862dd1526723246b20206e5fc37</span><br></pre></td></tr></table></figure><p>可以发现，手动计算出的hash值与<code>git hash-object</code>计算出来的一模一样。</p><p>在Git对象hash方法的注意事项中，提到<strong><code>header</code>中第二部分关于数据长度的计算，一定是字节的长度而不是字符串的长度</strong>。由于<code>what is up, doc?</code>只有英文字符，在UTF8中恰好字符的长度和字节的长度都等于16，很容易将这个长度误解为字符的长度。假设我们以<code>中文</code>来试验：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ echo -n &quot;中文&quot; | git hash-object --stdin</span><br><span class="line">efbb13322ba66f682e179ebff5eeb1bd6ef83972</span><br><span class="line">$ echo -n &quot;blob 2\0中文&quot; | openssl sha1</span><br><span class="line">d1dc2c3eed26b05289bddb857713b60b8c23ed29</span><br></pre></td></tr></table></figure><p>我们可以看到，<code>git hash-object</code>和<code>openssl sha1</code>计算出来的hash值根本不一样。这是因为<code>中文</code>两个字符作为UTF格式存储后的字符长度不是2，具体是多少呢？可以使用<code>wc</code>来计算：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ echo -n &quot;中文&quot; | wc -c</span><br><span class="line">       6</span><br></pre></td></tr></table></figure><p><code>中文</code>字符串的字节长度是6，重新手动计算发现得出的hash值就能对应上了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ echo -n &quot;blob 6\0中文&quot; | openssl sha1</span><br><span class="line">efbb13322ba66f682e179ebff5eeb1bd6ef83972</span><br></pre></td></tr></table></figure><h2 id="树对象"><a href="#树对象" class="headerlink" title="树对象"></a>树对象</h2><p>树对象的内容格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tree &lt;content length&gt;&lt;NUL&gt;&lt;file mode&gt; &lt;filename&gt;&lt;NUL&gt;&lt;item sha&gt;...</span><br></pre></td></tr></table></figure><p>需要注意的是，<code>&lt;item sha&gt;</code>部分是二进制形式的sha1码，而不是十六进制形式的sha1码。</p><p>我们从上一篇<a href="1">文章</a>摘出一个树对象做实验，其内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git cat-file -p d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br><span class="line">100644 blob 83baae61804e65cc73a7201a7252750c76066a30  test.txt</span><br></pre></td></tr></table></figure><p>我们首先使用<code>xxd</code>把<code>83baae61804e65cc73a7201a7252750c76066a30</code>转换成为二进制形式，并将结果保存为<code>sha1.txt</code>以方便后面做追加操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ echo -n &quot;83baae61804e65cc73a7201a7252750c76066a30&quot; | xxd -r -p &gt; sha1.txt</span><br><span class="line">$ cat tree-items.txt</span><br><span class="line">���a�Ne�s� rRu</span><br><span class="line">              vj0%</span><br></pre></td></tr></table></figure><p>接下来构造content部分，并保存至文件<code>content.txt</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ echo -n &quot;100644 test.txt\0&quot; | cat - sha1.txt &gt; content.txt</span><br><span class="line">$ cat content.txt</span><br><span class="line">100644 test.txt���a�Ne�s� rRu</span><br><span class="line">                             vj0%</span><br></pre></td></tr></table></figure><p>计算content的长度：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat content.txt | wc -c</span><br><span class="line">      36</span><br></pre></td></tr></table></figure><p>那么最终该树对象的内容为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ echo -n &quot;tree 36\0&quot; | cat - content.txt</span><br><span class="line">tree 36100644 test.txt���a�Ne�s� rRu</span><br><span class="line">                                    vj0%</span><br></pre></td></tr></table></figure><p>最后使用<code>openssl sha1</code>计算hash值，可以发现和实验的hash值是一样的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ echo -n &quot;tree 36\0&quot; | cat - content.txt | openssl sha1</span><br><span class="line">d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br></pre></td></tr></table></figure><h2 id="提交对象"><a href="#提交对象" class="headerlink" title="提交对象"></a>提交对象</h2><p>提交对象的格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">commit &lt;content length&gt;&lt;NUL&gt;tree &lt;tree sha&gt;</span><br><span class="line">parent &lt;parent sha&gt;</span><br><span class="line">[parent &lt;parent sha&gt; if several parents from merges]</span><br><span class="line">author &lt;author name&gt; &lt;author e-mail&gt; &lt;timestamp&gt; &lt;timezone&gt;</span><br><span class="line">committer &lt;author name&gt; &lt;author e-mail&gt; &lt;timestamp&gt; &lt;timezone&gt;</span><br><span class="line"></span><br><span class="line">&lt;commit message&gt;</span><br></pre></td></tr></table></figure><p>我们从上一篇<a href="1">文章</a>摘出一个提交对象做实验，其内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ echo &apos;first commit&apos; | git commit-tree d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br><span class="line">db1d6f137952f2b24e3c85724ebd7528587a067a</span><br><span class="line">$ git cat-file -p db1d6f137952f2b24e3c85724ebd7528587a067a</span><br><span class="line">tree d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br><span class="line">author jingsam &lt;jing-sam@qq.com&gt; 1528022503 +0800</span><br><span class="line">committer jingsam &lt;jing-sam@qq.com&gt; 1528022503 +0800</span><br><span class="line"></span><br><span class="line">first commit</span><br></pre></td></tr></table></figure><p>这里需要注意的是，由于<code>echo &#39;first commit&#39;</code>没有添加<code>-n</code>选项，因此实际的提交信息是<code>first commit\n</code>。使用<code>wc</code>计算出提交内容的字节数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ echo -n &quot;tree d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br><span class="line">author jingsam &lt;jing-sam@qq.com&gt; 1528022503 +0800</span><br><span class="line">committer jingsam &lt;jing-sam@qq.com&gt; 1528022503 +0800</span><br><span class="line"></span><br><span class="line">first commit\n&quot; | wc -c</span><br><span class="line">     163</span><br></pre></td></tr></table></figure><p>那么，这个提交对象的<code>header</code>就是<code>commit 163\0</code>，手动把头部添加到提交内容中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">commit 163\0tree d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br><span class="line">author jingsam &lt;jing-sam@qq.com&gt; 1528022503 +0800</span><br><span class="line">committer jingsam &lt;jing-sam@qq.com&gt; 1528022503 +0800</span><br><span class="line"></span><br><span class="line">first commit\n</span><br></pre></td></tr></table></figure><p>使用<code>openssl sha1</code>计算这个上面内容的hash值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ echo -n &quot;commit 163\0tree d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br><span class="line">author jingsam &lt;jing-sam@qq.com&gt; 1528022503 +0800</span><br><span class="line">committer jingsam &lt;jing-sam@qq.com&gt; 1528022503 +0800</span><br><span class="line"></span><br><span class="line">first commit\n&quot; | openssl sha1</span><br><span class="line">db1d6f137952f2b24e3c85724ebd7528587a067a</span><br></pre></td></tr></table></figure><p>可以看见，与实验的hash值是一样的。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这篇文章详细地分析了Git中的数据对象、树对象和提交对象的hash方法，可以发现原理是非常简单的。数据对象和提交对象打印出来的内容与存储内容组织是一模一样的，可以很直观的理解。对于树对象，其打印出来的内容和实际存储是有区别的，增加了一些实现上的难度。例如，使用二进制形式的hash值而不是直观的十六进制形式，我现在还没有从已有资料中搜到这么设计的理由，这个问题留待以后解决。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在上一篇&lt;a href=&quot;1&quot;&gt;文章&lt;/a&gt;中，将了数据对象、树对象和提交对象三种Git对象，每种对象会计算出一个hash值。那么，Git是如何计算出Git对象的hash值？本文的内容就是来解答这个问题。&lt;/p&gt;
&lt;h1 id=&quot;Git对象的hash方法&quot;&gt;&lt;a href=
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Git内部原理之Git对象</title>
    <link href="https://jingsam.github.io/2018/06/03/git-objects.html"/>
    <id>https://jingsam.github.io/2018/06/03/git-objects.html</id>
    <published>2018-06-03T09:50:55.000Z</published>
    <updated>2018-06-09T16:09:22.831Z</updated>
    
    <content type="html"><![CDATA[<p>最近在读《Pro Git》这本书，其中有一章讲Git的内部原理，写得非常好，读完之后对于Git的理解会提升到一个新的层次。今后，我会写一系列的关于Git内部原理的文章，以帮助读者加深对Git的认识。内容主要参考《Pro Git》这本书，但不同的是，我会对内容进行重新组织，以使大家更容易理解。</p><p>这篇文章的主题的Git对象。</p><p><strong>从根本上来讲，Git是一个内容寻址的文件系统，其次才是一个版本控制系统。</strong>记住这点，对于理解Git的内部原理及其重要。所谓“内容寻址的文件系统”，意思是根据文件内容的hash码来定位文件。这就意味着同样内容的文件，在这个文件系统中会指向同一个位置，不会重复存储。</p><p>Git对象包含三种：数据对象、树对象、提交对象。Git文件系统的设计思路与linux文件系统相似，即将文件的内容与文件的属性分开存储，文件内容以“装满字节的袋子”存储在文件系统中，文件名、所有者、权限等文件属性信息则另外开辟区域进行存储。在Git中，数据对象相当于文件内容，树对象相当于文件目录树，提交对象则是对文件系统的快照。</p><p>下面的章节，会分别对每种对象进行说明。开始说明之前，先初始化一个Git文件系统：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir git-test</span><br><span class="line">$ cd git-test</span><br><span class="line">$ git init</span><br></pre></td></tr></table></figure><p>接下来的操作都会在<code>git-test</code>这个目录中进行。</p><h1 id="数据对象"><a href="#数据对象" class="headerlink" title="数据对象"></a>数据对象</h1><p>数据对象是文件的内容，不包括文件名、权限等信息。Git会根据文件内容计算出一个hash值，以hash值作为文件索引存储在Git文件系统中。由于相同的文件内容的hash值是一样的，因此Git将同样内容的文件只会存储一次。<code>git hash-object</code>可以用来计算文件内容的hash值，并将生成的数据对象存储到Git文件系统中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ echo &apos;version 1&apos; | git hash-object -w --stdin</span><br><span class="line">83baae61804e65cc73a7201a7252750c76066a30</span><br><span class="line">$ echo &apos;version 2&apos; | git hash-object -w --stdin</span><br><span class="line">1f7a7a472abf3dd9643fd615f6da379c4acb3e3a</span><br><span class="line">$ echo &apos;new file&apos; | git hash-object -w --stdin</span><br><span class="line">fa49b077972391ad58037050f2a75f74e3671e92</span><br></pre></td></tr></table></figure><p>上面示例中，<code>-w</code>表示将数据对象写入到Git文件系统中，如果不加这个选项，那么只计算文件的hash值而不写入；<code>--stdin</code>表示从标准输入中获取文件内容，当然也可以指定一个文件路径代替此选项。</p><p>上面讲数据对象写入到Git文件系统中，那如何读取数据对象呢？<code>git cat-file</code>可以用来实现所有Git对象的读取，包括数据对象、树对象、提交对象的查看：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git cat-file -p 83baae61804e65cc73a7201a7252750c76066a30</span><br><span class="line">version 1</span><br><span class="line">$ git cat-file -t 83baae61804e65cc73a7201a7252750c76066a30</span><br><span class="line">blob</span><br></pre></td></tr></table></figure><p>上面示例中，<code>-p</code>表示查看Git对象的内容，<code>-t</code>表示查看Git对象的类型。</p><p>通过这一节，我们能够对Git文件系统中的数据对象进行读写。但是，我们需要记住每一个数据对象的hash值，才能访问到Git文件系统中的任意数据对象，这显然是不现实的。数据对象只是解决了文件内容存储的问题，而文件名的存储则需要通过下一节的树对象来解决。</p><h1 id="树对象"><a href="#树对象" class="headerlink" title="树对象"></a>树对象</h1><p>树对象是文件目录树，记录了文件获取目录的名称、类型、模式信息。使用<code>git update-index</code>可以为数据对象指定名称和模式，然后使用<code>git write-tree</code>将树对象写入到Git文件系统中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git update-index --add --cacheinfo 100644 83baae61804e65cc73a7201a7252750c76066a30 test.txt</span><br><span class="line">$ git write-tree</span><br><span class="line">d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br></pre></td></tr></table></figure><p><code>--add</code>表示新增文件名，如果第一次添加某一文件名，必须使用此选项；<code>--cacheinfo &lt;mode&gt; &lt;object&gt; &lt;path&gt;</code>是要添加的数据对象的模式、hash值和路径，<code>&lt;path&gt;</code>意味着为数据对象不仅可以指定单纯的文件名，也可以使用路径。另外要注意的是，使用<code>git update-index</code>添加完文件后，一定要使用<code>git write-tree</code>写入到Git文件系统中，否则只会存在于index区域。</p><p>树对象仍然可以使用<code>git cat-file</code>查看：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git cat-file -p d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br><span class="line">100644 blob 83baae61804e65cc73a7201a7252750c76066a30  test.txt</span><br><span class="line">$ git cat-file -t d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br><span class="line">tree</span><br></pre></td></tr></table></figure><p>上面表示这个树对象只有<code>test.txt</code>这个文件，接下来我们将<code>version 2</code>的数据对象指定为<code>test.txt</code>，并添加一个新文件<code>new.txt</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git update-index --cacheinfo 100644 1f7a7a472abf3dd9643fd615f6da379c4acb3e3a test.txt</span><br><span class="line">$ git update-index --add --cacheinfo 100644 fa49b077972391ad58037050f2a75f74e3671e92 new.txt</span><br><span class="line">$ git write-tree</span><br><span class="line">0155eb4229851634a0f03eb265b69f5a2d56f341</span><br></pre></td></tr></table></figure><p>查看树对象<code>0155eb</code>，可以发现这个树对象有两个文件了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git cat-file -p 0155eb4229851634a0f03eb265b69f5a2d56f341</span><br><span class="line">100644 blob fa49b077972391ad58037050f2a75f74e3671e92  new.txt</span><br><span class="line">100644 blob 1f7a7a472abf3dd9643fd615f6da379c4acb3e3a  test.txt</span><br></pre></td></tr></table></figure><p>我们甚至可以使用<code>git read-tree</code>，将已添加的树对象读取出来，作为当前树的子树：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git read-tree --prefix=bak d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br><span class="line">$ git write-tree</span><br><span class="line">3c4e9cd789d88d8d89c1073707c3585e41b0e614</span><br></pre></td></tr></table></figure><p><code>--prefix</code>表示把子树对象放到哪个目录下。查看树对象，可以发现当前树对象有一个文件夹和两个文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git cat-file -p 3c4e9cd789d88d8d89c1073707c3585e41b0e614</span><br><span class="line">040000 tree d8329fc1cc938780ffdd9f94e0d364e0ea74f579  bak</span><br><span class="line">100644 blob fa49b077972391ad58037050f2a75f74e3671e92  new.txt</span><br><span class="line">100644 blob 1f7a7a472abf3dd9643fd615f6da379c4acb3e3a  test.txt</span><br></pre></td></tr></table></figure><p>最终，整个树对象的结构如下图：</p><p><img src="/assets/2018-06-03-1.png" alt="树对象结构图"></p><p>树对象解决了文件名的问题，而且，由于我们是分阶段提交树对象的，树对象可以看做是开发阶段源代码目录树的一次次快照，因此我们可以是用树对象作为源代码版本管理。但是，这里仍然有问题需要解决，即我们需要记住每个树对象的hash值，才能找到个阶段的源代码文件目录树。在源代码版本控制中，我们还需要知道谁提交了代码、什么时候提交的、提交的说明信息等，接下来的提交对象就是为了解决这个问题的。</p><h1 id="提交对象"><a href="#提交对象" class="headerlink" title="提交对象"></a>提交对象</h1><p>提交对象是用来保存提交的作者、时间、说明这些信息的，可以使用<code>git commit-tree</code>来将提交对象写入到Git文件系统中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ echo &apos;first commit&apos; | git commit-tree d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br><span class="line">db1d6f137952f2b24e3c85724ebd7528587a067a</span><br></pre></td></tr></table></figure><p>上面<code>commit-tree</code>除了要指定提交的树对象，也要提供提交说明，至于提交的作者和时间，则是根据环境变量自动生成，并不需要指定。这里需要提醒一点的是，读者在测试时，得到的提交对象hash值一般和这里不一样，这是因为提交的作者和时间是因人而异的。</p><p>提交对象的查看，也是使用<code>git cat-file</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ git cat-file -p db1d6f137952f2b24e3c85724ebd7528587a067a</span><br><span class="line">tree d8329fc1cc938780ffdd9f94e0d364e0ea74f579</span><br><span class="line">author jingsam &lt;jing-sam@qq.com&gt; 1528022503 +0800</span><br><span class="line">committer jingsam &lt;jing-sam@qq.com&gt; 1528022503 +0800</span><br><span class="line"></span><br><span class="line">first commit</span><br></pre></td></tr></table></figure><p>上面是属于首次提交，那么接下来的提交还需要指定使用<code>-p</code>指定父提交对象，这样代码版本才能成为一条时间线：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ echo &apos;second commit&apos; | git commit-tree 0155eb4229851634a0f03eb265b69f5a2d56f341 -p db1d6f137952f2b24e3c85724ebd7528587a067a</span><br><span class="line">d4d2c6cffb408d978cb6f1eb6cfc70e977378a5c</span><br></pre></td></tr></table></figure><p>使用<code>git cat-file</code>查看一下新的提交对象，可以看到相比于第一次提交，多了<code>parent</code>部分：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ git cat-file -p d4d2c6cffb408d978cb6f1eb6cfc70e977378a5c</span><br><span class="line">tree 0155eb4229851634a0f03eb265b69f5a2d56f341</span><br><span class="line">parent db1d6f137952f2b24e3c85724ebd7528587a067a</span><br><span class="line">author jingsam &lt;jing-sam@qq.com&gt; 1528022722 +0800</span><br><span class="line">committer jingsam &lt;jing-sam@qq.com&gt; 1528022722 +0800</span><br><span class="line"></span><br><span class="line">second commit</span><br></pre></td></tr></table></figure><p>最后，我们再将树对象<code>3c4e9c</code>提交：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ echo &apos;third commit&apos; | git commit-tree 3c4e9cd789d88d8d89c1073707c3585e41b0e614 -p d4d2c6cffb408d978cb6f1eb6cfc70e977378a5c</span><br><span class="line">3ac728ac62f0a7b5ac201fd3ed1f69165df8be31</span><br></pre></td></tr></table></figure><p>使用<code>git log</code>可以查看整个提交历史：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">$ git log --stat 3ac728ac62f0a7b5ac201fd3ed1f69165df8be31</span><br><span class="line">commit 3ac728ac62f0a7b5ac201fd3ed1f69165df8be31</span><br><span class="line">Author: jingsam &lt;jing-sam@qq.com&gt;</span><br><span class="line">Date:   Sun Jun 3 18:47:29 2018 +0800</span><br><span class="line"></span><br><span class="line">    third commit</span><br><span class="line"></span><br><span class="line"> bak/test.txt | 1 +</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br><span class="line"></span><br><span class="line">commit d4d2c6cffb408d978cb6f1eb6cfc70e977378a5c</span><br><span class="line">Author: jingsam &lt;jing-sam@qq.com&gt;</span><br><span class="line">Date:   Sun Jun 3 18:45:22 2018 +0800</span><br><span class="line"></span><br><span class="line">    second commit</span><br><span class="line"></span><br><span class="line"> new.txt  | 1 +</span><br><span class="line"> test.txt | 2 +-</span><br><span class="line"> 2 files changed, 2 insertions(+), 1 deletion(-)</span><br><span class="line"></span><br><span class="line">commit db1d6f137952f2b24e3c85724ebd7528587a067a</span><br><span class="line">Author: jingsam &lt;jing-sam@qq.com&gt;</span><br><span class="line">Date:   Sun Jun 3 18:41:43 2018 +0800</span><br><span class="line"></span><br><span class="line">    first commit</span><br><span class="line"></span><br><span class="line"> test.txt | 1 +</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br></pre></td></tr></table></figure><p>最终的提交对象的结构如下图：</p><p><img src="/assets/2018-06-03-2.png" alt="提交对象结构示意图"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>Git中的数据对象解决了数据存储的问题，树对象解决了文件名存储问题，提交对象解决了提交信息的存储问题。从Git设计中可以看出，Linus对一个源代码版本控制系统做了很好的抽象和解耦，每种对象解决的问题都很明确，相比于使用一种数据结构，无疑更灵活和更易维护。每种Git对象都有一个hash值，这个值是怎么计算出来的？Git的各种对象是如何存储的？这些问题将在下一篇文章中讲解。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近在读《Pro Git》这本书，其中有一章讲Git的内部原理，写得非常好，读完之后对于Git的理解会提升到一个新的层次。今后，我会写一系列的关于Git内部原理的文章，以帮助读者加深对Git的认识。内容主要参考《Pro Git》这本书，但不同的是，我会对内容进行重新组织，以
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>手动更新Homebrew formula</title>
    <link href="https://jingsam.github.io/2018/05/23/edit-a-brew-formula.html"/>
    <id>https://jingsam.github.io/2018/05/23/edit-a-brew-formula.html</id>
    <published>2018-05-23T06:24:01.000Z</published>
    <updated>2018-05-28T14:49:32.330Z</updated>
    
    <content type="html"><![CDATA[<p>Homebrew是macOS上的软件包管理神器，类似于Ubuntu的apt-get，是使用mac作为开发机时的必装软件之一。Homebrew的软件包术语叫Formula，中文解释就是”配方”。Homebrew有“家酿、自制”的意思，80年代硅谷有一个著名的计算机俱乐部叫“Homebrew Homebrew Computer Club”，苹果的创始人Steve Jobs和Steve Wozniak都曾是这个俱乐部的活跃分子。“家酿”一个东西当然得有“配方”，所以这个取名很形象。</p><p>使用homebrew安装软件很方便，一条命令<code>brew install your-formula-name</code>就可以搞定。更新formula到最新版本，使用<code>brew upgrade your-formula-name</code>即可。</p><p>但是，各种formula是人工维护的，当软件包更新后，formula不见得能及时更新到最新版本。本文就以gdal为例，说明如何手动编辑formula文件，以此来将软件更新到最新版本。</p><h1 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h1><p>Homebrew中的每个软件包都是通过一个<code>formula.rb</code>文件来配置软件的源代码URL、依赖、编译规则和选项，例如以下是gdal的formula文件：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Gdal</span> &lt; Formula</span></span><br><span class="line">  desc <span class="string">"Geospatial Data Abstraction Library"</span></span><br><span class="line">  homepage <span class="string">"http://www.gdal.org/"</span></span><br><span class="line">  url <span class="string">"https://download.osgeo.org/gdal/2.2.4/gdal-2.2.4.tar.xz"</span></span><br><span class="line">  sha256 <span class="string">"6f75e49aa30de140525ccb58688667efe3a2d770576feb7fbc91023b7f552aa2"</span></span><br><span class="line">  revision <span class="number">2</span></span><br><span class="line"></span><br><span class="line">  bottle <span class="keyword">do</span></span><br><span class="line">    rebuild <span class="number">2</span></span><br><span class="line">    sha256 <span class="string">"00b28455769c3d5d6ea13dc119f213f320c247489cb2ce9d03f7791d4b53919b"</span> =&gt; <span class="symbol">:high_sierra</span></span><br><span class="line">    sha256 <span class="string">"1365de6a18caeb84d6a50e466a63be9c7541b1fab21edfc3012812157464f2c0"</span> =&gt; <span class="symbol">:sierra</span></span><br><span class="line">    sha256 <span class="string">"8c0fd81eda5a91c8a75a78795f96b6dd9c53e74974bd38cc004b55a44ae95932"</span> =&gt; <span class="symbol">:el_capitan</span></span><br><span class="line">  <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">  ....</span><br></pre></td></tr></table></figure><p>以上是一个ruby文件，好在并不需要懂ruby的语法就能看懂formula文件。<code>desc</code>和<code>homepage</code>都是描述性信息，不对软件安装产生什么影响。<code>url</code>是软件源代码的位置，编译安装时从此位置将源代码下载下来。<code>sha256</code>是源代码包的校验码，这是保证下载下来的包不被篡改。<code>revision</code>是修订版本号，主要用来保持版本号不变的情况下，对软件包打补丁，每打一次补丁，修订版本号就自增一次。当使用<code>brew install</code>安装软件包时，除非使用<code>--build-from-source</code>强制指定使用源代码安装，大部分情况下，brew会下载编译好的二进制包，这样安装起来更快。<code>bottle</code>选项就记录了各版本macOS下预编译的二进制包的校验码，这部分内容是homebrew的自动集成工具自动维护的，我们并不需要编辑修改。</p><p>更改上面的<code>url</code>和<code>sha256</code>，即可将formula的配置更新到任意版本。编辑好后，使用<code>brew install</code>或者<code>brew upgrade</code>进行安装或者更新升级。</p><p>Homebrew实际上是通过git来管理formula配置文件的，因此我们还可以发送Pull request，将我们的更新推送到GitHub上，让别人也能够方便地更新软件包。</p><h1 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h1><p>这部分以更新gdal 2.2.4到2.3.0为例，来说明手动更新formula的步骤。由于写作本文时，gdal已经更新到2.3.0，所以某些步骤的输出可能与本文不一致，但不妨碍理解更新步骤。</p><h2 id="编辑配置"><a href="#编辑配置" class="headerlink" title="编辑配置"></a>编辑配置</h2><p>使用<code>brew edit gdal</code>即可打开<code>gdal.rb</code>开始编辑，我们将<code>url</code>更新为2.3.0版本的源代码链接：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Gdal</span> &lt; Formula</span></span><br><span class="line">  desc <span class="string">"Geospatial Data Abstraction Library"</span></span><br><span class="line">  homepage <span class="string">"http://www.gdal.org/"</span></span><br><span class="line">  url <span class="string">"https://download.osgeo.org/gdal/2.3.0/gdal-2.3.0.tar.xz"</span></span><br><span class="line">  sha256 <span class="string">"6f75e49aa30de140525ccb58688667efe3a2d770576feb7fbc91023b7f552aa2"</span></span><br><span class="line">  revision <span class="number">2</span></span><br><span class="line"></span><br><span class="line">  ...</span><br></pre></td></tr></table></figure><p>理论上我们还需要更新<code>sha256</code>，使它和<code>url</code>相匹配。但是<code>sha256</code>需要我们使用工具计算或者从发布网站上找，不是很方便。我们可以通过下一步的安装调试，来自动计算<code>sha256</code>。</p><h2 id="安装调试"><a href="#安装调试" class="headerlink" title="安装调试"></a>安装调试</h2><p>使用<code>brew install gdal --verbose --debug --build-from-source</code>来安装调试gdal的formula，如果已经安装旧版本的gdal，那么使用<code>brew upgrade gdal --verbose --debug --build-from-source</code>。<code>--verbose</code>表示显示详细输出，便于调试；<code>--debug</code>打开调试；<code>--build-from-source</code>强制从源代码编译。</p><p>安装过程中，会报sha256校验码不匹配的警告，并打印出<code>url</code>所指向的源代码包的sha256校验值，这是因为在上一步我们并没有修改<code>sha256</code>，配置文件中<code>sha256</code>还是gdal 2.2.4版本的。这时候重新使用<code>brew edit gdal</code>打开并编辑formula文件，将<code>sha256</code>更改为正确的校验值。</p><p>最后，再安装调试，经过漫长的编译，成功地安装上了gdal的最新版本。</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>测试包括对软件包的测试和对formula文件的测试。使用<code>brew test gdal</code>可以测试gdal的功能是否正常，使用<code>brew audit --strict gdal</code>测试formula文件是否正确。运行<code>brew audit</code>时，会报告<code>revision 2</code>需被删除，这是因为当前homebrew线上库中还没有gdal 2.3.0，意味着本地端的gdal应该是第一版本，不存在修订版本之说。同样地，使用<code>brew edit gdal</code>打开并删除<code>revision</code>部分，然后再重新测试。</p><h2 id="推送更新"><a href="#推送更新" class="headerlink" title="推送更新"></a>推送更新</h2><p>通过上述步骤，我们完成了gdal的手动更新，如果将更新推送到homebrew的线上库中，那么其他人就可以方便地更新到最新版本。并且推送到线上库后，homebrew的自动集成工具会自动地编译生成二进制包，这样就不需要从源代码编译那么耗时了，可谓是利人利己。</p><p>由于homebrew是在GitHub上协作的，所以更新一个formula就和发一个Pull request是一样的，基本步骤如下：</p><p>1.使用<code>cd $(brew --repository homebrew/homebrew-core)</code>切换到本地的homebrew-core目录；<br>2.使用<code>git commit</code>提交自己的修改；<br>3.把<a href="https://github.com/Homebrew/homebrew-core" target="_blank" rel="noopener">https://github.com/Homebrew/homebrew-core</a> fork一份；<br>4.使用<code>git remote add</code>命令添加自己的fork的homebrew-core库；<br>5.使用<code>git push</code>推送将本地提交推送到自己的fork的homebrew-core库中；<br>6.在GitHub网页上发起Pull request。</p><h2 id="一键更新"><a href="#一键更新" class="headerlink" title="一键更新"></a>一键更新</h2><p>上面一步步完成了编辑配置、安装调试、测试、推送更新，操作起来有些繁琐。但其实homebrew还提供了一个工具，能够一键完成上面4个步骤，命令如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew bump-formula-pr gdal --URL=https://download.osgeo.org/gdal/2.3.0/gdal-2.3.0.tar.xz --audit --strict</span><br></pre></td></tr></table></figure><p><code>brew bump-formula-pr</code>自动的修改formula配置文件、检查文件错误、提交并推送更新，其中提交并推送更新的过程需要使用<code>hub</code>来在终端上操作GitHub，可以使用<code>brew install hub</code>来安装这个工具。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Homebrew是macOS上的软件包管理神器，类似于Ubuntu的apt-get，是使用mac作为开发机时的必装软件之一。Homebrew的软件包术语叫Formula，中文解释就是”配方”。Homebrew有“家酿、自制”的意思，80年代硅谷有一个著名的计算机俱乐部叫“H
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>linux进程后台运行</title>
    <link href="https://jingsam.github.io/2018/05/07/linux-nohup.html"/>
    <id>https://jingsam.github.io/2018/05/07/linux-nohup.html</id>
    <published>2018-05-07T09:25:15.000Z</published>
    <updated>2018-05-21T07:17:40.753Z</updated>
    
    <content type="html"><![CDATA[<p>在linux上启动Web服务，当退出终端后，Web服务进程也会随着关闭。产生这种问题的原因在于，当用户注销或者网络断开后，终端后收到挂断信号（SIGHUP）,并向子进程广播SIGHUP信号，子进程收到SIGHUP信号而关闭。因此，让linux后台持续运行的方法有以下几种：</p><p>1.改变子进程的所属的父进程，只要父进程不关闭，子进程也不会关闭；<br>2.让子进程忽略挂断信号，即使收到SIGHUP信号，也任性地继续运行；<br>3.不向子进程广播SIGHUP信号，子进程收不到SIGHUP信号，因而不会关闭。</p><h1 id="第一种方式"><a href="#第一种方式" class="headerlink" title="第一种方式"></a>第一种方式</h1><p>使用<code>setsid</code>可以新开一个session运行进程，此session不从属于当前终端，因此终端关闭时进程也不会退出。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;setsid ping www.baidu.com</span><br><span class="line">&gt;ps -ef | grep www.baidu.com</span><br><span class="line">501 57697     1   0  5:55下午 ttys000    0:00.01 ping www.baidu.com</span><br></pre></td></tr></table></figure><p>从上面可以看出，ping进程的父进程是1，即init进程，因此只要电脑不关机，ping进程就不会停止。</p><p>linux下自带<code>setsid</code>这个命令，但是macOS上并没有这个命令。此时，可以结合使用<code>()</code>和<code>&amp;</code>达到同样的效果。<code>()</code>可以新开一个subshell，<code>&amp;</code>让命令后台运行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;(ping www.baidu.com &amp;)</span><br><span class="line">&gt;ps -ef | grep www.baidu.com</span><br><span class="line">501 57781     1   0  6:01下午 ttys000    0:00.00 ping www.baidu.com</span><br></pre></td></tr></table></figure><p>可以看到，效果与<code>setsid</code>是一样的。</p><h1 id="第二种方式"><a href="#第二种方式" class="headerlink" title="第二种方式"></a>第二种方式</h1><p>使用<code>nohup</code>可以使进程忽略SIGHUP信号，这种方式也是最常用的。例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;nohup ping www.baidu.com &amp;</span><br><span class="line">[1]  + 59193 exit 127   nohup www.baidu.com</span><br><span class="line"></span><br><span class="line">&gt;ps -ef | grep www.baidu.com</span><br><span class="line">501 59193 39100   0  6:05下午 ttys000    0:00.01 ping www.baidu.com</span><br></pre></td></tr></table></figure><p>可以看到，ping进程的父进程并不为1，nohup是让进程忽略SIGHUP信号实现进程不退出的。</p><p>需要注意的是，当在<code>zsh</code>中使用<code>nohup</code>时，退出终端时会提示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zsh: you have running jobs.</span><br></pre></td></tr></table></figure><p>再次强行退出，那么进程仍然会被干掉。这时候，采用以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup ping www.baidu.com &amp;!</span><br></pre></td></tr></table></figure><h1 id="第三种方式"><a href="#第三种方式" class="headerlink" title="第三种方式"></a>第三种方式</h1><p>使用nohup的前提是，进程以nohup来启动。但是，如果启动时忘记了以nohup启动，有什么方法在不停止进程的情况，让它继续后台运行呢？接下来就要将另外一个命令：<code>disown</code>。<code>disown</code>的原理是，将子进程从终端任务队列中移除，所以即使终端挂断，子进程也收不到SIGHUP信号。</p><p>假设现在使用ping命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;ping www.baidu.com</span><br><span class="line">64 bytes from 14.215.177.38: icmp_seq=0 ttl=51 time=19.642 ms</span><br><span class="line">64 bytes from 14.215.177.38: icmp_seq=1 ttl=51 time=97.976 ms</span><br><span class="line">64 bytes from 14.215.177.38: icmp_seq=2 ttl=51 time=88.996 ms</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>这时采用<code>Ctrl + z</code>使它进入后台，使用<code>jobs</code>查看后台进程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;jobs</span><br><span class="line">[1]  + suspended  ping www.baidu.com</span><br></pre></td></tr></table></figure><p>可以看到虽然ping进程进入后台，但是进程被挂起了，没有继续运行。使用<code>bg</code>命令可以使他在后台继续运行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;bg %1</span><br><span class="line">[1]  + 58174 continued  ping www.baidu.com</span><br><span class="line">&gt;jobs</span><br><span class="line">[1]  + running    ping www.baidu.com</span><br></pre></td></tr></table></figure><p>通过组合<code>Ctrl + z</code>和<code>bg</code>，成功地将前台进程变为了后台进程。为了让进程不随终端关闭而终止，还差最后一步：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;disown %1</span><br><span class="line">&gt;jobs</span><br></pre></td></tr></table></figure><p>上面使用<code>jobs</code>命令查看任务队列，发现ping进程不在任务队列中，意味着进程不会收到SIGHUP信号。</p><h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><p>以上我们通过三种方式，避免进程随着终端关闭而被杀掉：</p><ol><li><code>setsid</code>改变父进程，只要父进程不关闭，进程就可以持续运行；</li><li><code>nohup</code>使得进程忽略SIGHUP信号，父进程即使发送挂断信号，进程也不会终止；</li><li><code>disown</code>将进程从任务队列中移除，保证进程收不到SIGHUP信号。</li></ol><p>但是，以上种种方法只是避免了进程受到SIGHUP信号的影响，进程的持续运行还需要一些其他环境，例如stdin、stdout以及stderr。通常从终端启动的进程，会继承终端的stdin、stdout和stderr。当终端断掉之后，stdin、stdout和stderr也会随着消失，若此时后台进程需要读写stdin、stdout、stderr，该进程将会暂停或者挂住。所以，为保证进程正常后台运行，最好启动时对输入输出重定向：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;ping www.baidu.com &gt; a.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>此时，将stdout和stderr重定向到文件<code>a.log</code>中，文件<code>a.log</code>不受终端关闭的影响。如果进程依赖于stdin，意思是进程需要由于键盘输入，那就说明这是个交互式程序，交互式程序后台运行就没多大意义了。</p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://www.ibm.com/developerworks/cn/linux/l-cn-nohup/index.html" target="_blank" rel="noopener">Linux 技巧：让进程在后台可靠运行的几种方法</a><br><a href="https://unix.stackexchange.com/questions/3886/difference-between-nohup-disown-and" target="_blank" rel="noopener">Difference between nohup, disown and &amp;</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在linux上启动Web服务，当退出终端后，Web服务进程也会随着关闭。产生这种问题的原因在于，当用户注销或者网络断开后，终端后收到挂断信号（SIGHUP）,并向子进程广播SIGHUP信号，子进程收到SIGHUP信号而关闭。因此，让linux后台持续运行的方法有以下几种：&lt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Windows Server 2012评估版本延长使用期限</title>
    <link href="https://jingsam.github.io/2018/03/13/windows-server-rearm.html"/>
    <id>https://jingsam.github.io/2018/03/13/windows-server-rearm.html</id>
    <published>2018-03-13T07:00:51.000Z</published>
    <updated>2018-03-13T07:24:31.129Z</updated>
    
    <content type="html"><![CDATA[<p>最近一台Windows Server 2012 R2数据库服务器隔一个小时就关机，恰巧碰到要演示，这种突发情况让我手忙脚乱。（别问我为什么要用Windows Server做服务器，客户要求的）网上搜了下，发现是Windows授权到期了，需要激活。</p><p>网上确实有很多Windows Server破解激活工具，但是有各种各样的工具，不知道哪个能起作用。而且，这些工具大部分杀毒软件都报有病毒，不太敢用。最重要的是，破解激活涉及到版权问题，道义上过不去。</p><p>由于我使用的是Windows Server评估版本，评估版本可以重置5次试用期，所以加上安装的那次，那么理论了最多可以试用1080天，差不多3年。</p><p>重置方法很简单，以管理员身份打开命令提示符，输入以下命令即可重置试用期，获得180天试用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slmgr.vbs /rearm</span><br></pre></td></tr></table></figure><p><strong>重置成功后需要重启一次才能生效。</strong></p><p>通过以下命令可以查看剩余的试用时间和可重置次数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slmgr.vbs /dlv</span><br></pre></td></tr></table></figure><p>那么重置次数试用完之后怎么办呢？据说可以通过改注册表的方式获得额外的重置次数，但是这种方法可能违反了系统试用协议。具体的做法是使用<code>regedit</code>打开注册表编辑器，找到：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HKEY_LOCAL_MACHINE -&gt; SOFTWARE -&gt; Microsoft -&gt; Windows NT -&gt; CurrentVersion -&gt; SoftwareProtectionPlatform</span><br></pre></td></tr></table></figure><p>将键<code>SkipRearm</code>的值设为1，再用<code>slmgr.vbs /rearm</code>继续重置，据说这种方法可以使用8次。</p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="http://www.vixual.net/blog/archives/180" target="_blank" rel="noopener">Windows Server 2012 評估版與延長使用期限</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近一台Windows Server 2012 R2数据库服务器隔一个小时就关机，恰巧碰到要演示，这种突发情况让我手忙脚乱。（别问我为什么要用Windows Server做服务器，客户要求的）网上搜了下，发现是Windows授权到期了，需要激活。&lt;/p&gt;
&lt;p&gt;网上确实有很
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>关于package.json中main字段的指向问题</title>
    <link href="https://jingsam.github.io/2018/03/12/npm-main.html"/>
    <id>https://jingsam.github.io/2018/03/12/npm-main.html</id>
    <published>2018-03-12T11:58:22.000Z</published>
    <updated>2018-03-13T07:30:29.576Z</updated>
    
    <content type="html"><![CDATA[<p><code>package.json</code>中的<code>main</code>字段指向的是Library的入口，通常有3个选择：</p><p>1.指向源代码入口文件，如<code>src/index.js</code>;2.指向打包后的开发版本，如<code>dist/library.js</code>;3.指向打包后的发布版本，如<code>dist/library.min.js</code>。</p><p>引用Library的方式也分为两种：</p><p>1.通过script标签直接引用，适用于简单页面；<br>2.通过require或import方式引用，需要借助打包工具打包，适用于复杂页面。</p><p>本文探讨一下<code>main</code>字段如何指定，才能兼顾各种引用方式。</p><h1 id="指向源代码入口文件"><a href="#指向源代码入口文件" class="headerlink" title="指向源代码入口文件"></a>指向源代码入口文件</h1><p>第一种方式指向源码入口，这种情况仅适用于require方式引用。由于指向的是源代码，需要库使用者借助打包工具如webpack，自行对库进行打包。此方式存在以下问题：</p><p>1.webpack配置babel-loader一般会排除node_modules，意味着不会对library进行转译，可能会导致打包后的代码中包含ES6代码，造成低版本浏览器兼容问题；<br>2.如果library的编译需要一些特别的loader或loader配置，使用者需要在自己的配置中加上这些配置，否则会造成编译失败；<br>3.使用者的打包工具需要收集library的依赖，造成打包编译速度慢，影响开发体验。</p><p>总的来说，第一种方式需要使用者自行对library进行编译打包，对使用者造成额外的负担，因此源代码入口文件不适宜作为库的入口。但是，如果library的目标运行环境只是node端，由于node端不需要对源代码进行编译打包，所以这种情况下可以使用<code>src/index.js</code>作为库入口。</p><h1 id="指向打包后的开发版本"><a href="#指向打包后的开发版本" class="headerlink" title="指向打包后的开发版本"></a>指向打包后的开发版本</h1><p>开发版本的主要作用是便于调试，文件体积并不是开发版本所关心的问题，这是因为开发版本通常是托管在localhost上，文件大小基本没影响。</p><p>开发版本主要通过以下手段来方便调试，提升开发体验：</p><p>1.预先进行依赖收集和babel转译，即使用者不再需要对library进行这两步工作了，提高编译打包的效率；<br>2.尽量保留源代码的格式，保证开发版本里面的源代码基本可读；<br>3.保留警告信息，对开发者对库的错误或不合理调用进行提示。</p><p>其中第3点是通过库代码中添加如下类似代码实现的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if (process.env.NODE_ENV === &apos;development&apos;) &#123;</span><br><span class="line">  console.warn(&apos;Some useful warnings.&apos;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>生成开发版本的似乎，webpack的DefinePlugin会将<code>process.env.NODE_ENV</code>替换为<code>development</code>，所以以上代码变为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if (&apos;development&apos; === &apos;development&apos;) &#123;</span><br><span class="line">  console.warn(&apos;Some useful warnings.&apos;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这就表示上述条件一直成立，warning信息会显示出来。</p><p>最近和iview的开发者争论一件事，即在生成library的开发版本的时候，<code>NODE_ENV</code>应该设置为<code>development</code>还是<code>production</code>。他们认为应该设置为<code>production</code>，理由是可以减小开发版本的体积。假设DefinePlugin将<code>process.env.NODE_ENV</code>替换为<code>production</code>，之前的示例代码会变为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if (&apos;production&apos; === &apos;development&apos;) &#123;</span><br><span class="line">  console.warn(&apos;Some useful warnings.&apos;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这就意味着你使用库开发应用时，不会看到任何警告信息，这不利于提前发现错误。可能有的人会说，我的源代码中没有<code>if (process.env.NODE_ENV === &#39;development&#39;) {}</code>这类代码，所以设置为<code>production</code>也不会有任何问题呀。殊不知，虽然你的源代码中这种没有这类提示代码，但是你的devependencies里面可能会有啊，这样做就会关闭依赖中的warning信息。</p><p>可能又有疑问：“引用开发版本的包体积很大，岂不是让我的应用打包上线版本很大？”其实完全不用担心，因为应用打包为上线版本时，会经过两个额外的工作：</p><p>1.使用DefinePlugin将<code>process.env.NODE_ENV</code>替换为<code>production</code>，关闭所有警告信息；<br>2.使用UglifyJsPlugin对应用代码进行minify，减小应用体积。同时会删除<code>if (&#39;production&#39; === &#39;development&#39;) {}</code>这类永远不会执行的代码，进一步减小应用体积。</p><p>所以，在开发时应用开发版本，不必担心最后的应用体积。但是如果开发时是以script标签的方式引用库的开发版本，上线时应该替换为响应的发布版本。</p><h1 id="指向打包后的发布版本"><a href="#指向打包后的发布版本" class="headerlink" title="指向打包后的发布版本"></a>指向打包后的发布版本</h1><p>发布版本追求的是尽量减小体积，因为相比于JS引擎解析的时间，网络传输是最慢的，所以要通过减小库的体积，减少网络传输的时间。</p><p>减小发布版本的文件体积，主要是通过将<code>process.env.NODE_ENV</code>设置为<code>production</code>，然后再使用UglifyJsPlugin对应用代码进行minify以及删除永不执行的代码。</p><p>那么将库的发布版本作为入口文件合不合适呢？显然不合适，因为发布版本的是经过高度压缩精简的，代码完全不可读，应用开发阶段难以调试。</p><p>发布版本是适用于在应用上线时，通过script标签形式引用。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>通过上面的分析，可以发现将库的开发版本作为库的入口才是正确合理的做法，即设置<code>&quot;main&quot;: &quot;dist/library.js&quot;</code>。而作为库的开发者，也要遵循约定，生成库的开发版本的时候，使用<code>development</code>环境变量，保留警告信息。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;code&gt;package.json&lt;/code&gt;中的&lt;code&gt;main&lt;/code&gt;字段指向的是Library的入口，通常有3个选择：&lt;/p&gt;
&lt;p&gt;1.指向源代码入口文件，如&lt;code&gt;src/index.js&lt;/code&gt;;
2.指向打包后的开发版本，如&lt;code&gt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Library项目的Webpack配置</title>
    <link href="https://jingsam.github.io/2018/03/06/webpack.html"/>
    <id>https://jingsam.github.io/2018/03/06/webpack.html</id>
    <published>2018-03-06T06:24:48.000Z</published>
    <updated>2018-03-07T09:03:12.306Z</updated>
    
    <content type="html"><![CDATA[<p>知乎上有个提问，叫“如何成为高级Webpack配置工程师”，戏谑Webpack已经复杂到成为一门专业学问了。但Webpack确实是非常复杂的，一般人只能做到入门而无法精通。Webpack的复杂性在于要完成各种各样的功能，即不仅要处理js、css、html、图片、字体等各种格式的前端资源，还要对这些资源进行转译、精简、提取、分割、打包等一系列操作。</p><p>Webpack在当前前端工程化中占有很重要的地位，前端工程化是为了提高前端开发的效率，但是前端工程化中的工具链配置的复杂度也逐渐提高。有时候新开一个项目，光是配置这些工具就要花一两天，这对于小型项目有点得不偿失。在配置工具链的时间花费大，诚然webpack本身配置参数多，我认为更重要的原因在于平时过于依赖于vue-cli、react-scripts这类自动化生成工具，没有具体地了解每个配置项的作用。</p><p>最近要开发一个可视化的JS库，但是vue-cli、react-scripts这类自动化生成工具主要针对的是SPA，对开发Library支持不好，因而尝试下手动配置webpack、eslint、babel、prettier这些工具。好在开发的是Library，主要处理JS代码，如果是SPA，那就需要处理各种各样的前端资源，还是建议使用vue-cli、react-scripts。</p><p>本文主要是做步骤记录，用于指导以后进行简单项目的手动配置，因而本篇文章不探讨深度内容。</p><h1 id="生成package-json"><a href="#生成package-json" class="headerlink" title="生成package.json"></a>生成package.json</h1><p>这块没什么好说的，借助<code>npm init</code>或<code>yarn init</code>可以快速地生成<code>package.json</code>文件。我通常习惯于先在<code>scripts</code>中把主要的开发命令写出来，以下是我的<code>scripts</code>配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;scripts&quot;: &#123;</span><br><span class="line">    &quot;build-dev&quot;: &quot;webpack --config build/webpack.dev.config.js&quot;,</span><br><span class="line">    &quot;build-prod&quot;: &quot;webpack --config build/webpack.prod.config.js&quot;,</span><br><span class="line">    &quot;build&quot;: &quot;npm run build-dev &amp;&amp; npm run build-prod&quot;,</span><br><span class="line">    &quot;start&quot;: &quot;webpack-dev-server --config build/webpack.dev.config.js&quot;,</span><br><span class="line">    &quot;lint&quot;: &quot;eslint src/*.js&quot;,</span><br><span class="line">    &quot;format&quot;: &quot;prettier-eslint --write src/*.js&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的<code>scripts</code>配置也体现了下文要讲的工程目录结构，即<code>build</code>里面放webpack配置文件，<code>src</code>里面放源代码，详细的内容在下一节描述。</p><p><code>main</code>默认是<code>index.js</code>，即指向源代码的入口文件。但是本项目主要开发的库是作为其他项目的node_modules，一般不会再对库进行转译，所以为了方便将本库集成到前段工程项目中，<code>main</code>应该指向转译好的UMD格式文件。本项目的<code>main</code>配置为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;main&quot;: &quot;dist/geoeye.js&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="规划目录结构"><a href="#规划目录结构" class="headerlink" title="规划目录结构"></a>规划目录结构</h1><p>我认为工程的目录结构非常重要，它能够反映代码的模块划分，好的目录结构让人赏心悦目、容易理解。我按照以下目录进行组织：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">build  // 编译配置</span><br><span class="line">  |- webpack.dev.config.js</span><br><span class="line">  |- webpack.prod.config.js</span><br><span class="line">dist  // 编译好的库文件</span><br><span class="line">  |- geoeye.js</span><br><span class="line">  |- geoeye.min.js</span><br><span class="line">  |- geoeye.min.js.map</span><br><span class="line">src   // 源代码</span><br><span class="line">debug // 用于调试</span><br><span class="line">test  // 测试</span><br><span class="line">package.json</span><br></pre></td></tr></table></figure><h1 id="配置webpack"><a href="#配置webpack" class="headerlink" title="配置webpack"></a>配置webpack</h1><p>Webpack 4刚发布，据说简化了配置，所以本项目就来尝尝鲜。Webpack 4相比原来需要额外安装一个webpack-cli，并且要求node版本不小于6.11.5，安装命令如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save-dev webpack webpack-cli</span><br></pre></td></tr></table></figure><p>接下来就要配置<code>webpack.dev.config.js</code>和<code>webpack.prod.config.js</code>。webpack官方文档推荐用一个<code>webpack.common.config.js</code>提取公共配置后，再用<code>webpack-merge</code>合并。由于本项目配置比较简单，重复的地方不多，所以就不用引入额外的复杂度了。如果项目的配置复杂到同一种配置需要重复3次以上，那么还是需要采用<code>webpack-merge</code>合并的，因为同时更改3个地方很容易出错。</p><p>本项目的<code>webpack.dev.config.js</code>的配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">const path = require(&apos;path&apos;)</span><br><span class="line">const webpack = require(&apos;webpack&apos;)</span><br><span class="line"></span><br><span class="line">module.exports = &#123;</span><br><span class="line">  entry: &apos;./src/index.js&apos;,</span><br><span class="line">  output: &#123;</span><br><span class="line">    path: path.join(__dirname, &apos;../dist&apos;),</span><br><span class="line">    filename: &apos;geoeye.js&apos;,</span><br><span class="line">    library: &apos;geoeye&apos;,</span><br><span class="line">    libraryTarget: &apos;umd&apos;,</span><br><span class="line">    umdNamedDefine: true</span><br><span class="line">  &#125;,</span><br><span class="line">  devtool: &apos;cheap-module-eval-source-map&apos;,</span><br><span class="line">  devServer: &#123;</span><br><span class="line">    contentBase: &apos;./debug&apos;,</span><br><span class="line">    publicPath: &apos;/dist/&apos;,</span><br><span class="line">    hot: true,</span><br><span class="line">    open: true,</span><br><span class="line">    overlay: true</span><br><span class="line">  &#125;,</span><br><span class="line">  module: &#123;</span><br><span class="line">    rules: [</span><br><span class="line">      &#123;</span><br><span class="line">        test: /\.js$/,</span><br><span class="line">        loader: &apos;babel-loader&apos;,</span><br><span class="line">        exclude: /node_modules/</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;,</span><br><span class="line">  mode: &apos;development&apos;,</span><br><span class="line">  plugins: [</span><br><span class="line">    new webpack.HotModuleReplacementPlugin()</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>本项目的<code>webpack.prod.config.js</code>的配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">const path = require(&apos;path&apos;)</span><br><span class="line">const webpack = require(&apos;webpack&apos;)</span><br><span class="line"></span><br><span class="line">module.exports = &#123;</span><br><span class="line">  entry: &apos;./src/index.js&apos;,</span><br><span class="line">  output: &#123;</span><br><span class="line">    path: path.join(__dirname, &apos;../dist&apos;),</span><br><span class="line">    filename: &apos;geoeye.min.js&apos;,</span><br><span class="line">    library: &apos;geoeye&apos;,</span><br><span class="line">    libraryTarget: &apos;umd&apos;,</span><br><span class="line">    umdNamedDefine: true</span><br><span class="line">  &#125;,</span><br><span class="line">  devtool: &apos;source-map&apos;,</span><br><span class="line">  module: &#123;</span><br><span class="line">    rules: [</span><br><span class="line">      &#123;</span><br><span class="line">        test: /\.js$/,</span><br><span class="line">        loader: &apos;babel-loader&apos;,</span><br><span class="line">        exclude: /node_modules/</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;,</span><br><span class="line">  mode: &apos;production&apos;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以上有以下几个地方需要注意：</p><p>1.<code>entry</code>中使用相对于当前目录时，<code>./</code>不能省略，即<code>./src/index.js</code>不能写为<code>src/index.js</code>。2.<code>output</code>中的<code>path</code>一定要是绝对路径；<br>3.<code>libraryTarget</code>设为<code>umd</code>以兼容浏览器和commonjs环境；<br>4.webpack 4新引入了<code>mode</code>配置，会自动做一些优化，可以为<code>development</code>或<code>production</code>，不能省略<code>mode</code>的配置；<br>5.<code>mode</code>为<code>development</code>时，HotModuleReplacementPlugin不是默认载入的，所以为了使开发时候能够热替换，需要手动加上这个配置；</p><h1 id="配置babel"><a href="#配置babel" class="headerlink" title="配置babel"></a>配置babel</h1><p>babel负责将高语言特性JS源代码转译为低语言特性JS代码，以兼容低版本浏览器，当前推荐采用<code>babel-preset-env</code>，它能根据要兼容的浏览器版本，有选择性地转译，而不是像以前一样统统转译为ES5。</p><p>使用以下命令安装babel以及配套工具：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save-dev babel-core babel-preset-env babel-loader</span><br></pre></td></tr></table></figure><p><code>.babelrc</code>配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;presets&quot;: [</span><br><span class="line">    [</span><br><span class="line">      &quot;env&quot;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;targets&quot;: &#123;</span><br><span class="line">          &quot;browsers&quot;: [&quot;last 2 versions&quot;, &quot;ie 11&quot;]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="配置eslint和prettier"><a href="#配置eslint和prettier" class="headerlink" title="配置eslint和prettier"></a>配置eslint和prettier</h1><p>eslint能够检查源代码中的格式错误以及少量的语法错误，prettier是用来自动地格式化代码。它们的主要区别在于，eslint主要用来检查代码格式，prettier主要用来修复代码格式。虽然<code>eslint --fix</code>也能自动修复一些格式错误，但只能修复少数几种格式错误，功能十分有限。prettier的格式修复功能很强，但是如果代码中有错误，例如有尾逗号、引用未知变量，prettier不管这些，仍然帮你格式化，这就让你很难提早发现代码中的错误。</p><p>eslint和prettier相爱相杀，让它们和谐相处，才能更好地为我们提供服务。总体思想是eslint的检查规则尽量与prettier的格式规则保持一致，代码先用prettier格式化之后再用<code>eslint --fix</code>修复并检查。</p><p>需要先安装一下几个包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install --save-dev eslint prettier eslint-config-prettier eslint-plugin-prettier prettier-eslint-cli</span><br></pre></td></tr></table></figure><p><code>eslint-config-prettier</code>是用来将prettier的格式化规则作为eslint的检查规则，<code>eslint-plugin-prettier</code>则是用来对比prettier格式化前后，代码中出现的错误。<code>prettier-eslint-cli</code>是用来依次执行prettier和<code>eslint --fix</code>，自动格式化代码。</p><p><code>.eslintrc</code>的配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;env&quot;: &#123;</span><br><span class="line">    &quot;browser&quot;: true,</span><br><span class="line">    &quot;es6&quot;: true</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;parserOptions&quot;: &#123;</span><br><span class="line">    &quot;sourceType&quot;: &quot;module&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;extends&quot;: [&quot;prettier&quot;],</span><br><span class="line">  &quot;plugins&quot;: [&quot;prettier&quot;],</span><br><span class="line">  &quot;rules&quot;: &#123; &quot;prettier/prettier&quot;: &quot;error&quot; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>需要指出的是如果使用了ES6的<code>import</code>和<code>export</code>，则需要配置<code>&quot;sourceType&quot;: &quot;module&quot;</code>。</p><p>我是无分号党，<code>.prettierrc</code>配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;singleQuote&quot;: true,</span><br><span class="line">  &quot;semi&quot;: false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="配置-gitignore"><a href="#配置-gitignore" class="headerlink" title="配置.gitignore"></a>配置.gitignore</h1><p><code>.gitignore</code>用来排除不需要git管理的文件，配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">.DS_Store</span><br><span class="line">node_modules/</span><br><span class="line">dist/</span><br><span class="line">npm-debug.log*</span><br><span class="line">yarn-debug.log*</span><br><span class="line">yarn-error.log*</span><br><span class="line"></span><br><span class="line"># Editor directories and files</span><br><span class="line">.idea</span><br><span class="line">.vscode</span><br><span class="line">*.suo</span><br><span class="line">*.ntvs*</span><br><span class="line">*.njsproj</span><br><span class="line">*.sln</span><br></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>配置一个Library开发环境，分为生成package.json、规划目录结构、配置webpack、配置babel、配置eslint和prettier、配置.gitignore几个步骤。本文仅仅是流水账记录，深度不够，写到后面我都觉得乏味了，以后不写这种水文章了，匿了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;知乎上有个提问，叫“如何成为高级Webpack配置工程师”，戏谑Webpack已经复杂到成为一门专业学问了。但Webpack确实是非常复杂的，一般人只能做到入门而无法精通。Webpack的复杂性在于要完成各种各样的功能，即不仅要处理js、css、html、图片、字体等各种格
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>使用pkg打包Node.js应用</title>
    <link href="https://jingsam.github.io/2018/03/02/pkg.html"/>
    <id>https://jingsam.github.io/2018/03/02/pkg.html</id>
    <published>2018-03-02T03:23:39.000Z</published>
    <updated>2018-03-07T04:04:59.530Z</updated>
    
    <content type="html"><![CDATA[<p>Node.js应用不需要经过编译过程，可以直接把源代码拷贝到部署机上执行，确实比C++、Java这类编译型应用部署方便。然而，Node.js应用执行需要有运行环境，意味着你需要先在部署机器上安装Node.js。虽说没有麻烦到哪里去，但毕竟多了一个步骤，特别是对于离线环境下的部署机，麻烦程度还要上升一级。假设你用Node.js写一些小的桌面级工具软件，部署到客户机上还要先安装Node.js，有点“大炮打蚊子”的感觉。更严重的是，如果部署机器上游多个Node.js应用，而且这些应用要依赖于不同的Node.js版本，那就更难部署了。</p><p>理想的情况是将Node.js打包为一个单独的可执行文件，部署的时候直接拷贝过去就行了。除了部署方便外，因为不需要再拷贝源代码了，还有利于保护知识产权。</p><p>将Node.js打包为可执行文件的工具有pkg、nexe、node-packer、enclose等，从打包速度、使用便捷程度、功能完整性来说，pkg是最优秀的。这篇文章就来讲一讲半年来我使用pkg打包Node.js应用的一些经验。</p><p>pkg的打包原理简单来说，就是将js代码以及相关的资源文件打包到可执行文件中，然后劫持<code>fs</code>里面的一些函数，使它能够读到可执行文件中的代码和资源文件。例如，原来的<code>require(&#39;./a.js&#39;)</code>会被劫持到一个虚拟目录<code>require(&#39;/snapshot/a.js&#39;)</code>。</p><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>pkg既可以全局安装也可以局部安装，推荐采用局部安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install pkg --save-dev</span><br></pre></td></tr></table></figure><h1 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h1><p>pkg使用比较简单，执行下<code>pkg -h</code>就可以基本了解用法，基本语法是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pkg [options] &lt;input&gt;</span><br></pre></td></tr></table></figure><p><code>&lt;input&gt;</code>可以通过三种方式指定：</p><p>1.一个脚本文件，例如<code>pkg index.js</code>;2.<code>package.json</code>，例如<code>pkg package.json</code>，这时会使用<code>package.json</code>中的<code>bin</code>字段作为入口文件；<br>3.一个目录，例如<code>pkg .</code>，这时会寻找指定目录下的<code>package.json</code>文件，然后在找<code>bin</code>字段作为入口文件。</p><p><code>[options]</code>中可以指定打包的参数：<br>1.<code>-t</code>指定打包的目标平台和Node版本，如<code>-t node6-win-x64,node6-linux-x64,node6-macos-x64</code>可以同时打包3个平台的可执行程序；<br>2.<code>-o</code>指定输出可执行文件的名称，但如果用<code>-t</code>指定了多个目标，那么就要用<code>--out-path</code>指定输出的目录；<br>3.<code>-c</code>指定一个JSON配置文件，用来指定需要额外打包脚本和资源文件，通常使用<code>package.json</code>配置。</p><p>使用pkg的最佳实践是：在<code>package.json</code>中的<code>pkg</code>字段中指定打包参数，使用<code>npm scripts</code>来执行打包过程，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  ...</span><br><span class="line">  &quot;bin&quot;: &quot;./bin/www&quot;,</span><br><span class="line">  &quot;scripts&quot;: &#123;</span><br><span class="line">    &quot;pkg&quot;: &quot;pkg . --out-path=dist/&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;pkg&quot;: &#123;</span><br><span class="line">    &quot;scripts&quot;: [...]</span><br><span class="line">    &quot;assets&quot;: [...],</span><br><span class="line">    &quot;targets&quot;: [...]</span><br><span class="line">  &#125;,</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>scripts</code>和<code>assets</code>用来配置未打包进可执行文件的脚本和资源文件，文件路径可以使用glob通配符。这里就浮现出一个问题：为什么有的脚本和资源文件打包不进去呢？</p><p>要回答这个问题，就涉及到pkg打包文件的机制。按照pkg文档的说法，pkg只会自动地打包相对于<code>__dirname</code>、<code>__filename</code>的文件，例如<code>path.join(__dirname, &#39;../path/to/asset&#39;)</code>。至于<code>require()</code>，因为require本身就是相对于<code>__dirname</code>的，所以能够自动打包。假设文件中有以下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">require(&apos;./build/&apos; + cmd + &apos;.js&apos;)</span><br><span class="line">path.join(__dirname, &apos;views/&apos; + viewName)</span><br></pre></td></tr></table></figure><p>这些路径都不是常量，pkg没办法帮你自动识别要打包哪个文件，所以文件就丢失了，所以这时候就使用<code>scripts</code>和<code>assets</code>来告诉pkg，这些文件要打包进去。那么我们怎么知道哪些文件没有被打包呢？难倒要一行行地去翻源代码吗？其实很简单，只需要把打包好的文件运行下，报错信息一般就会告诉你缺失哪些文件，并且pkg在打包过程中也会提示一些它不能自动打包的文件。</p><h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><p>如果说pkg还有哪儿还可以改进的地方，那就是无法自动打包二进制模块<code>*.node</code>文件。如果你的项目中引用了二进制模块，如sqlite3，那么你需要手动地将<code>*.node</code>文件复制到可执行文件同一目录，我通常使用命令<code>cp node_modules/**/*.node .</code>一键完成。但是，如果你要跨平台打包，例如在windows上打包linux版本，相应的二进制模块也要换成linux版本，通常需要你手动的下载或者编译。</p><p>那为什么pkg不能将二进制模块打包进去呢？我猜想是require载入一个js文件和node文件，它们的机制是不一样的。另外从设计来说，不自动打包二进制模块也是合理的，因为二进制模块都是平台相关的。如果我在windows上生成一个linux文件，那么就需要拉取linux版本的<code>.node</code>文件，这是比较困难的。并且有些二进制模块不提供预编译版本，需要安装的时候编译，pkg再牛也不可能模拟一个其他平台的编译环境吧。nexe可以自动打包二进制模块，但是只能打包当前平台和当前版本的可执行文件。这意味着如果Node.js应用引用了二进制包，那么这个应用就不能跨平台打包了，所以我认为这方面，nexe不能算是一个好的设计。</p><p>还有一点就是关于项目中的配置文件处理，比如数据库连接参数、环境变量等。因为这些配置文件会跟着不同的部署环境进行更改，所以为了方便更改，一般不希望把配置文件打包到exe。为了避免pkg自动地将配置文件打包到exe中，代码中不要采用以下方式读取配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fs.readFile(path.join(__dirname, &apos;./config.json&apos;)), callback)</span><br></pre></td></tr></table></figure><p>而是采用相对于<code>process.CWD()</code>的方法读取：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fs.readFile(path.join(process.CWD(), &apos;./config.json&apos;), callback)</span><br><span class="line"></span><br><span class="line">// 或者</span><br><span class="line">fs.readFile(&apos;./config.json&apos;, callback)</span><br></pre></td></tr></table></figure><p>如果配置文件是js格式的，那么不要直接<code>require(&#39;./config&#39;)</code>，而是采用动态require：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">const config = require(process.CWD() + &apos;./config&apos;)</span><br></pre></td></tr></table></figure><p>另外要提及的是pkg打包之后动态载入js文件会有安全性问题，即用户可以在js文件写任何处理逻辑，注入到打包后的exe中。例如，可以读取exe里面的虚拟文件系统，把源代码导出来。所以，尽量不要采用JS作为配置文件，也不要动态载入js模块。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Node.js应用不需要经过编译过程，可以直接把源代码拷贝到部署机上执行，确实比C++、Java这类编译型应用部署方便。然而，Node.js应用执行需要有运行环境，意味着你需要先在部署机器上安装Node.js。虽说没有麻烦到哪里去，但毕竟多了一个步骤，特别是对于离线环境下的
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>《ThinkPad之道》书评</title>
    <link href="https://jingsam.github.io/2018/02/27/thinkpad.html"/>
    <id>https://jingsam.github.io/2018/02/27/thinkpad.html</id>
    <published>2018-02-27T03:27:47.000Z</published>
    <updated>2018-02-27T08:46:47.771Z</updated>
    
    <content type="html"><![CDATA[<p>最近在西西弗书店断断续续地读完了《ThinkPad之道》这本书，本书讲的是“Thinkpad之父”内藤在正和他所领导的大和实验室，在研发ThinkPad系列笔记本电脑的过程所面临的种种困难和压力，最终成就了一款经典产品。</p><p>本人大学期间拆装过无数笔记本电脑，自认为笔记本电脑没什么技术含量，无非就是几个零件组装起来。记得有一次拆装一个ThinkPad电脑，每次复原后总是会多出来几个螺丝，神奇的是每次电脑都能正常开机。心中不免有些疑惑，明明可以用更少的螺丝，为什么还要用那么多螺丝呢？看了这本书之后，明白了这些多余的螺丝，是为了保证ThinkPad在任何环境下都能正常使用。</p><p>ThinkPad起初是给哈佛商学院的学生使用的，后来才逐步拓展到其他大学和商务市场。刚开始时，ThinkPad在大学的故障率非常高，甚至需要专门在大学建立售后服务中心，应对大学生笔记本返修问题。内藤在正通过在大学中实地调研，发现大学生对ThinkPad的折磨程度超过了他的想象。例如，大学生会把装着ThinkPad的书包垫在台阶上，直接坐上去，由于小红点和屏幕只有2毫米的间隙，过重的压力会导致屏幕破裂；也会将ThinkPad放入装有十几公斤书本的书包中，骑着单车，让ThinkPad随意地翻滚；甚至在嬉戏玩耍的时候，将装有ThinkPad的书包扔来扔去。</p><p>为了让ThinkPad能够承受住大学生的“折磨”，大和实验室专门成立了测试ThinkPad承受能力的“酷刑室”。例如，有将笔记本开合一万次的仪器，用来测试笔记本铰链的质量；有将笔记本放在一个满是灰尘的空间中，看能不能正常使用；有将各种牛奶、果汁泼溅到笔记本键盘上，模拟使用时有液体溅入的场景。</p><p>最终，他们克服了种种困难，让ThinkPad成为了“稳定、可靠、质量”的代名词。靠着优秀的品质，ThinkPad进入过太空，深入过雨林，攀登过珠峰，游历过纳米比亚沙漠。</p><p>内藤在正和他的团队持续不断地对ThinkPad的改进，是一个典型的工匠精神的体现。然而，这么优秀的ThinkPad，为什么会被IBM卖给联想呢？因为IBM的PC部门一直在亏损。而且，生产核心器件如CPU、硬盘存储、屏幕的部门早已变卖，所以IBM认为PC这条产品线已经没有了核心竞争力，利润提不上去，卖给联想是明智的选择。</p><p>ThinkPad转手给联想之后，联想保留了日本大和实验室和美国罗利实验室，可以说保存了ThinkPad的核心团队。从我的观察来看，联想接手ThinkPad之后，一个主要的贡献是让人人都能用得起“小黑”（ThinkPad）。曾几何时，“小黑”是和“小白”（MacBook）一级的高端电脑，动辄几万块，一般人用不起。ThinkPad的质量确实好，大和实验室想尽千万种方法，提高ThinkPad的可靠性。这种不断进取的工匠精神从一些方面是好的，然而也带来了一些弊端。内藤在正也承认，他们可以在某一方面的技术做好精益求精，但是缺少宏观的视野，即所谓的“见木不见林”。不断地提高ThinkPad的可靠性，一方面这些可靠性需要有额外的零部件和加工方法支撑，因而增加了成本，导致ThinkPad的售价居高不下；另一方面，这些可靠性有些过度了，毕竟不是每台ThinkPad都是要“上刀山下火海”的。联想接手后，降低ThinkPad的成本，扩宽ThinkPad的价格范围，进军个人和家庭消费市场。现在，一个普通大学生拿4000块钱也可以买得到ThinkPad，真正让人人都用得起ThinkPad。</p><p>有段时间，ThinkPad的老用户抱怨，现在的“小黑”已经不是原来的“小黑”，话语中透露出失望的语气。但是，我认为“小黑”的精神没有丢，高端的ThinkPad产品仍然可靠，而中低端的ThinkPad产品也有着“小黑”的影子。现在无处不在的ThinkPad表明，ThinkPad在联想的经营下，焕发了新的活力。</p><p>ThinkPad的经历，不禁让我思考，到底什么才是好的产品？IBM时代的ThinkPad，可靠耐用，从技术上说是好的产品，但从商业角度和消费者的角度来说，不是好的产品，因为价格太贵了。联想时代的ThinkPad，牺牲了一些可靠性，换来了价格的降低，个人消费得起，可靠性方面也能满足日常使用，这时候ThinkPad才成为了好的产品。</p><p>本人同为技术人员，遇到挑战往往有一股狠劲，不解决不罢休，但容易钻牛角尖，缺乏大局观。现实世界的问题，不是都能靠技术来解决的。一款好的产品，不见得需要技术上先进，关键是要让消费者以可接受的价格满足他们的需求。所以，不能埋头只做技术，要倾听消费者的意见、进行商业上的考量、做技术上的取舍，这样才能做出好的产品。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近在西西弗书店断断续续地读完了《ThinkPad之道》这本书，本书讲的是“Thinkpad之父”内藤在正和他所领导的大和实验室，在研发ThinkPad系列笔记本电脑的过程所面临的种种困难和压力，最终成就了一款经典产品。&lt;/p&gt;
&lt;p&gt;本人大学期间拆装过无数笔记本电脑，自认
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>2018计划</title>
    <link href="https://jingsam.github.io/2018/02/26/2018-plan.html"/>
    <id>https://jingsam.github.io/2018/02/26/2018-plan.html</id>
    <published>2018-02-26T02:10:28.000Z</published>
    <updated>2018-02-26T03:38:18.416Z</updated>
    
    <content type="html"><![CDATA[<p>过去一年，发现自己越来越难以下笔写东西，原因是身体和思想上的双重懒惰。常常是心中拟定了很多写作的题目，而到要下笔写的时候，总是觉得我对这个主题研究还不够深入全面，然后就默默地为自己找到一个借口：“等我把这个研究透之后再下笔写”，最终就不了了之了。</p><p>博文对于我而言，是一种对技术的总结和沉淀。回望2017，自己的博文写得很少，心中很是失落。失落在于自己的心太浮躁，对技术没有进行总结和沉淀。</p><p>我下决心在2018年要多写文章，在此立下flag：“2018年每月要产出4篇文章，全年要有48篇文章上线”。要实现这个目标，首先“等我把这个研究透之后再下笔写”这种借口就不成立了，想到某个主题直接写就是了，深度不够？再来一篇。其二，时间上要保障，自己要把逛社交网站、看新闻的时间，抽出来作为写作的时间。三是注意力要集中，不能什么技术研究到一半，注意力又转移到其他技术上了。</p><p>关于写作的主题范围，主要还是矢量瓦片方面的技术，包括mapbox-gl-js、矢量切片工具、矢量瓦片服务以及矢量瓦片在时空大数据可视化方面的应用。</p><p>关于个人的技术成长，总来的说，要强核心、补短板、宽视野。“强核心”是要强化自己在在线制图和大数据可视化方面的积累，这是自己的核心竞争力，看家老本不能丢。“补短板”是补全自己在Web开发方面的不足，主要是CSS、WebGL以及对MVVM模式的认识，这方面需要自己系统地阅读书籍以及流行库的源代码。“宽视野”是要扩大自己的技术范围，自己不能局限于某一项技术上，要广泛涉猎，知道各项技术的原理，能够敏锐地察觉到某项技在地理信息中的应用前景。</p><p>最后，以上的目标要实现的前提是，自己要加强时间的管理。过去的一年，自己没闲着，一直在忙项目上的事情。但从心底来说，对于去年自己的成果，不甚满意，我认为本来还可以完成更多事情。究其原因，自己在时间的安排、任务优先级的把控方面很是不足。所以，今年迫切要改掉坏习惯，把自己的时间规划好。具体做法还是多动笔杆子，把自己的计划事项写下来，这样自己才能不断地排优先级、做计划、实施以及最后的回顾。</p><p>总而言之，“凡事预则立，不预则废”。2018，我要做一个有计划的人。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;过去一年，发现自己越来越难以下笔写东西，原因是身体和思想上的双重懒惰。常常是心中拟定了很多写作的题目，而到要下笔写的时候，总是觉得我对这个主题研究还不够深入全面，然后就默默地为自己找到一个借口：“等我把这个研究透之后再下笔写”，最终就不了了之了。&lt;/p&gt;
&lt;p&gt;博文对于我而
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>mapbox-gl-js接入GeoJSON大文件的优化策略</title>
    <link href="https://jingsam.github.io/2017/10/23/large-geojson-with-mapbox-gl-js.html"/>
    <id>https://jingsam.github.io/2017/10/23/large-geojson-with-mapbox-gl-js.html</id>
    <published>2017-10-23T01:35:06.000Z</published>
    <updated>2017-11-02T02:33:34.106Z</updated>
    
    <content type="html"><![CDATA[<p>mapbox-gl-js可以接受GeoJSON数据，在前端动态地绘制GeoJSON数据。需要说明的是，mapbox-gl-js并不能直接渲染GeoJSON数据，而是通过<code>geojson-vt</code>这个库，在前端动态地将GeoJSON数据转换为矢量瓦片后渲染。至于为什么不支持直接渲染GeoJSON数据，我猜想是因为mapbox-gl-js对于点、线、面、符号、注记等要素的绘制规则都是基于矢量瓦片的，为GeoJSON另外实现一套绘制规则，不仅增大了实现成本，而且两套规则可能会出现冲突。</p><p>传统的矢量数据需要预先切片、存储、发布服务，并且当数据更新时又得重新切片，对于小项目来说有点大炮打蚊子的感觉。GeoJSON数据源的好处是接入容易、更新方便，并且市面上有很多工具可以将各种矢量数据转换为GeoJSON格式。</p><p>通常来说，GeoJSON数据源适合于少量的矢量数据，大批量的矢量数据最好还是预先切片。但多少数据算是”少量数据呢“？从经验上讲，应该将GeoJSON数据控制在20M以下。需要说明的是，之所以提出”20M“这个经验值，并不是因为mapbox-gl-js对于渲染超过20M的GeoJOSN数据有困难，而是综合考虑网络请求时间和解析数据的时间。实际上，mapbox-gl-js处理100M的GeoJSON数据都没有什么问题，但是把大量的GeoJSON数据整体请求过来所产生的耗时，会严重影响用户体验。所以对于大量的GeoJSON数据，最好预先切片，这样前端可以分块请求，减少网络请求时间和解析数据的时间。</p><p>虽然使用GeoJSON大文件作为数据源不是最佳选择，但有些情况下我们不得不使用GeoJSON作为数据源，例如数据是由第三方通过API提供的、数据是实时更新的。当接入大GeoJSON数据时，我们可以通过以下几种优化策略，提高地图绘制的效率。</p><h1 id="使用cluster选项"><a href="#使用cluster选项" class="headerlink" title="使用cluster选项"></a>使用cluster选项</h1><p>如果需要接入的数据是点数据，并且十分密集，这时应该对点进行聚合。特别密集的点在小缩放级别会重叠在一起，造成注记压盖叠加，可视化效果不好。使用聚合后，小的缩放级别显示聚合数据，大的缩放级别显示原始数据，兼顾了综合和细节。并且，聚合减少了单张瓦片的数据量，使得引擎绘制更快，交互更流畅。</p><p>mapbox-gl-js是通过<code>supercluster</code>这个库对GeoJSON数据进行聚合。其原理是两个点的显示距离小于聚合半径时，聚合为一个点要素，这个点会有一个<code>point_count</code>字段记录聚合的点的数目。需要注意的是，并不是每个层级上所有的点都被聚合，有些孤立的点会保留原位。那么如何区分聚合的和未聚合的点要素呢？可以使用有没有<code>point_cluster</code>进行判断，即使用<code>&quot;filter&quot;: [&quot;has&quot;, &quot;point_count&quot;]</code>过滤出聚合的点要素，使用<code>&quot;filter&quot;: [&quot;!has&quot;, &quot;point_count&quot;]</code>过滤出未聚合的点要素。</p><p>mapbox-gl-js通过<code>cluster</code>、<code>clusterRadius</code>、<code>clusterMaxZoom</code>三个参数控制点聚合的效果，并且是要在<code>source</code>里面配置。从逻辑上讲，这三个是控制聚合的效果的，应该放到<code>layer</code>上更符合直觉。然而，mapbox-gl-js的聚合过程并不是在渲染层实现的，而是在数据层实现的，即在动态切片的时候对数据做了聚合操作。所以，从实现角度来讲，将聚合控制参数放在<code>source</code>是合情合理的。以下是这三个参数的说明：</p><ul><li><code>cluster</code>：设为<code>true</code>开启GeoJSON数据聚合，只对点数据有效；</li><li><code>clusterRaius</code>：聚合半径，默认为50像素。当你发现有很多点没有被聚合时，可以适当调大这个参数；</li><li><code>clusterMaxZoom</code>：聚合的最大层级，大于这个层级的数据不再聚合，默认值是<code>maxzoom - 1</code>。</li></ul><h1 id="减小buffer"><a href="#减小buffer" class="headerlink" title="减小buffer"></a>减小buffer</h1><p>矢量瓦片一般会向边界外围扩张一定距离，作为缓冲区，以免边界上的接边要素出现断开的现象。缓冲区越大，出现断裂的可能性越小，但同时导致每张瓦片的尺寸变大，渲染的效率也更低。可以适当减小缓冲区的大小，减小瓦片的额体积，提高渲染效率。</p><p><code>buffer</code>的默认值是128像素，能够应付大部分情况。当GeoJSON是点数据，并且用来渲染点要素的符号和注记文本宽度比较小时，可以尝试将<code>buffer</code>设置为0或者稍大的值。</p><h1 id="减小maxzoom"><a href="#减小maxzoom" class="headerlink" title="减小maxzoom"></a>减小maxzoom</h1><p><code>maxzoom</code>用来控制将GeoJSON数据切割成矢量瓦片的最大级别，默认值是18级。减小<code>maxzoom</code>，可以加快切片的速度。一般来说，切割到12级，可以保证一定的精度和速度，满足大部分应用需求。</p><h1 id="对数据进行minify"><a href="#对数据进行minify" class="headerlink" title="对数据进行minify"></a>对数据进行minify</h1><p>对GeoJSON数据进行minify，即移除JSON中多余的空格、换行符、注释，以及减少坐标点的小数位，可以减小GeoJSON的体积，减小加载时间。</p><h1 id="使用URL加载数据"><a href="#使用URL加载数据" class="headerlink" title="使用URL加载数据"></a>使用URL加载数据</h1><p>使用GeoJSON作为数据源，既可以将GeoJSON数据内联到样式文件中，也可以通过URL引用。从直觉上讲，使用内联的GeoJSON不需要网络请求，应该比URL方式更快，然而并不是这样的。使用内联的方式加载大量GeoJSON数据，有以下几个弊端：</p><ol><li>使样式文件变大，增加了解析的时间；</li><li>内联的GeoJSON数据解析后无法释放，将会一直占用内存。</li></ol><p>因此，对于大GeoJSON数据，尽量以URL的方式加载，可以减少客户端的内存占用。</p><h1 id="渲染时增大minzoom"><a href="#渲染时增大minzoom" class="headerlink" title="渲染时增大minzoom"></a>渲染时增大minzoom</h1><p>在配置GeoJSON渲染图层时，默认的<code>minzoom</code>是0，可以适当增大<code>minzoom</code>。<code>geojson-vt</code>在动态切片时，默认小于5级的瓦片是按需生成，大于或等于5级是预先生成。所以，但渲染图层的<code>minzoom</code>大于等于5时，小层级的瓦片不会生成，减少了计算负担。另外，增大<code>minzoom</code>，可以在小层级不需要加载、解析、渲染瓦片，所以渲染更为流畅。</p><h1 id="渲染时允许符号重叠"><a href="#渲染时允许符号重叠" class="headerlink" title="渲染时允许符号重叠"></a>渲染时允许符号重叠</h1><p>在配置GeoJSON渲染图层时，可以设置允许注记符号重叠，即设置<code>icon-allow-overlap: true</code>，提高渲染的效率。</p><p>mapbox-gl-js在渲染注记符号和文本时，会有一个碰撞检测的过程，即当两个注记有重叠，则隐藏一个。常规条件下，这个碰撞检测很快，耗时可以忽略不计。但是，如果数据中的点非常密集，这部分耗时还是比较客观的。所以，设置<code>icon-allow-overlap: true</code>关闭这个检测过程，可以提高渲染效率。</p><p>当然，可以进一步设置允许注记文本也可以重叠，即设置<code>text-allow-overlap: true</code>，继续提高渲染效率。然而，从用户的角度来说，地图上出现符号重叠可以忍受，文本重叠就比较难看了。所以，一般不推荐设置允许注记文本重叠。</p><h1 id="对数据进行切分"><a href="#对数据进行切分" class="headerlink" title="对数据进行切分"></a>对数据进行切分</h1><p>如果GeoJSON数据实在很大，可以将GeoJSON数据切分为几块，分别加载。比如，原来是一个GeoJSON大文件，现在把它切分为两个比较小的GeoJSON文件，可以提高处理的效率，原因在于：</p><ol><li>切分的数据可以并行请求。例如，请求两个5M的数据，比单独请求一个10M的数据要快；</li><li><code>geojson-vt</code>处理小文件比处理大文件快。</li></ol><h1 id="预先切片"><a href="#预先切片" class="headerlink" title="预先切片"></a>预先切片</h1><p>如果GeoJSON数据太大了，比如100M，以上的种种优化方法都不可行。GeoJSON没办法增量加载，必须将完整的GeoJSON数据请求过来，才能开始处理。大的GeoJSON数据，光网络请求就会占用大量时间，所以必须预先切片，分块加载。</p><p>预先切片有诸多好处，如减少网络请求时间、渲染效率高。但是，预先切片需要工具切片、瓦片服务器发布服务，而且数据更新有点麻烦。到底切不切片，是动态切片还是预先切片，需要根据数据的情况来确定。</p><p>你可以将数据上传到Mapbox Studio进行自动切片，也可以使用开源工具<a href="https://github.com/mapbox/tippecanoe" target="_blank" rel="noopener">tippecanoe</a>对GeoJSON数据自行切片。</p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol><li><a href="https://blog.mapbox.com/rendering-big-geodata-on-the-fly-with-geojson-vt-4e4d2a5dd1f2" target="_blank" rel="noopener">https://blog.mapbox.com/rendering-big-geodata-on-the-fly-with-geojson-vt-4e4d2a5dd1f2</a></li><li><a href="https://blog.mapbox.com/clustering-millions-of-points-on-a-map-with-supercluster-272046ec5c97" target="_blank" rel="noopener">https://blog.mapbox.com/clustering-millions-of-points-on-a-map-with-supercluster-272046ec5c97</a></li><li><a href="https://gist.github.com/ryanbaumann/2d5c851aebf46e4ef5702ee29ead6bdb" target="_blank" rel="noopener">https://gist.github.com/ryanbaumann/2d5c851aebf46e4ef5702ee29ead6bdb</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;mapbox-gl-js可以接受GeoJSON数据，在前端动态地绘制GeoJSON数据。需要说明的是，mapbox-gl-js并不能直接渲染GeoJSON数据，而是通过&lt;code&gt;geojson-vt&lt;/code&gt;这个库，在前端动态地将GeoJSON数据转换为矢量瓦片后渲染
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>mapbox-gl如何高效地高亮要素</title>
    <link href="https://jingsam.github.io/2017/10/20/mapbox-gl-highlight.html"/>
    <id>https://jingsam.github.io/2017/10/20/mapbox-gl-highlight.html</id>
    <published>2017-10-20T00:42:01.000Z</published>
    <updated>2017-10-20T08:46:42.452Z</updated>
    
    <content type="html"><![CDATA[<p>mapbox-gl基于矢量瓦片的前端渲染技术，使得要素高亮变得简单。要素高亮具体如何实现，有如下三种方法：</p><h1 id="第一种：动态过滤"><a href="#第一种：动态过滤" class="headerlink" title="第一种：动态过滤"></a>第一种：动态过滤</h1><p>这种方法的基本思路是添加一个高亮图层，然后根据鼠标hover的要素id，动态地改变filter条件，实现要素的高亮。</p><p>使用到的主要接口是<code>map.on(&#39;mousemove&#39;, layer, e)</code>，其中<code>e</code>可以获取到当前鼠标位置的features，效果如下：</p><iframe width="100%" height="500" src="//jsfiddle.net/jingsam/f3au0qLo/4/embedded/result,js,html,css" frameborder="0"></iframe><p>测试中发现，在图斑比较密集的情况下，高亮非常卡，滞后严重，效率并不高。</p><h1 id="第二种：数据源镜像"><a href="#第二种：数据源镜像" class="headerlink" title="第二种：数据源镜像"></a>第二种：数据源镜像</h1><p>第二种方法是对第一种方法的改进，思路如下：对原始数据源做一个镜像，即添加一个新的数据源，名称不通但指向的是同一套数据,例如下面示例中的<code>source-mirror</code>，高亮图层的数据源指向<code>source-mirror</code>。改进后的效果如下：</p><iframe width="100%" height="500" src="//jsfiddle.net/jingsam/rj16bqa4/4/embedded/result,js,html,css" frameborder="0"></iframe><p>测试可以发现，稍微改进一下，高亮的效率提升很大。</p><p>但为什么做一个数据源镜像就可以显著地提高高亮的效率呢？我猜想是mapbox-gl在绘制时，会按照数据源对图层分组。指向同一个数据源的图层组中，任意一个图层的渲染条件改变，将会以整个图层组为粒度重新进行运算。在第一种方法中，高亮图层和其他图层都指向同一个数据源，动态地改变高亮图层的过滤条件，导致了很大的运算开销。而第二种方法，为高亮图层单独分配一个数据源，动态地改变高亮图层的过滤条件，也只会导致高亮图层的重绘。</p><h1 id="第三种：动态生成高亮数据源"><a href="#第三种：动态生成高亮数据源" class="headerlink" title="第三种：动态生成高亮数据源"></a>第三种：动态生成高亮数据源</h1><p>我们能不能再进一步提高高亮的效率呢？有。第三种方法的思路是：为高亮图层生成一个空的GeoJSON数据源，然后将鼠标hover到的要素动态地填充到数据源中。</p><p>使用的接口只要是<code>map.getSource(source).setData(geojson)</code>，其中<code>getSource</code>用于根据sourceId获取数据源，<code>setData</code>用于动态更新数据。效果如下：</p><iframe width="100%" height="500" src="//jsfiddle.net/jingsam/ykoyet0w/4/embedded/result,js,html,css" frameborder="0"></iframe><p>测试可以发现，这种方法甚至比第二种方法效率更高。原因在于要高亮的要素往往很小，前段切成瓦片可能就一两张，因此不用去整体从原始数据源的瓦片种过滤，效率会更高。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>第一种方法最常用，小批量数据效率还行，但涉及到大数据量情况下，效率就不太理想了。</p><p>第二种方法通过小小的改进，就可以极大地提高效率，实现起来也很简单。</p><p>第三种方法效率最高，但其中引入了<code>turf.union</code>去合并features，带来了额外的依赖。因此，从简单和优雅的角度来说，我更推荐使用第二种方法。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;mapbox-gl基于矢量瓦片的前端渲染技术，使得要素高亮变得简单。要素高亮具体如何实现，有如下三种方法：&lt;/p&gt;
&lt;h1 id=&quot;第一种：动态过滤&quot;&gt;&lt;a href=&quot;#第一种：动态过滤&quot; class=&quot;headerlink&quot; title=&quot;第一种：动态过滤&quot;&gt;&lt;/a&gt;第
      
    
    </summary>
    
    
  </entry>
  
</feed>
